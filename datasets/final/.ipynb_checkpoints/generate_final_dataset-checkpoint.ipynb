{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad2cd5c0-0f46-468e-95b5-da8622b46c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:105: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 2,691  |  Columnas canónicas: 33\n",
      "\n",
      "Top 20 columnas con más missing (%):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>dtype</th>\n",
       "      <th>missing_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>has_github</td>\n",
       "      <td>float64</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>has_reddit</td>\n",
       "      <td>float64</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>has_telegram</td>\n",
       "      <td>float64</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ico_start_date</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>max_investment_raw</td>\n",
       "      <td>float64</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>min_investment_raw</td>\n",
       "      <td>float64</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rating</td>\n",
       "      <td>float64</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>team_size</td>\n",
       "      <td>float64</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>website_available</td>\n",
       "      <td>float64</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>whitelist</td>\n",
       "      <td>float64</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>whitepaper_available</td>\n",
       "      <td>float64</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>max_investment_usd</td>\n",
       "      <td>float64</td>\n",
       "      <td>99.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>min_investment_usd</td>\n",
       "      <td>float64</td>\n",
       "      <td>99.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tokens_for_sale</td>\n",
       "      <td>float64</td>\n",
       "      <td>97.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>accepts</td>\n",
       "      <td>object</td>\n",
       "      <td>97.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>token_price_usd</td>\n",
       "      <td>float64</td>\n",
       "      <td>96.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>total_tokens</td>\n",
       "      <td>float64</td>\n",
       "      <td>96.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amount_raised_usd</td>\n",
       "      <td>float64</td>\n",
       "      <td>93.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>interest</td>\n",
       "      <td>object</td>\n",
       "      <td>92.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ico_end_date</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>92.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  column           dtype  missing_%\n",
       "21            has_github         float64     100.00\n",
       "23            has_reddit         float64     100.00\n",
       "22          has_telegram         float64     100.00\n",
       "2         ico_start_date  datetime64[ns]     100.00\n",
       "32    max_investment_raw         float64     100.00\n",
       "31    min_investment_raw         float64     100.00\n",
       "26                rating         float64     100.00\n",
       "25             team_size         float64     100.00\n",
       "24     website_available         float64     100.00\n",
       "17             whitelist         float64     100.00\n",
       "30  whitepaper_available         float64     100.00\n",
       "14    max_investment_usd         float64      99.48\n",
       "13    min_investment_usd         float64      99.48\n",
       "12       tokens_for_sale         float64      97.62\n",
       "20               accepts          object      97.36\n",
       "10       token_price_usd         float64      96.99\n",
       "11          total_tokens         float64      96.69\n",
       "6      amount_raised_usd         float64      93.65\n",
       "27              interest          object      92.60\n",
       "3           ico_end_date  datetime64[ns]      92.57"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Guardado canónico: final/ico_union_canonical.csv\n",
      "CPU times: total: 406 ms\n",
      "Wall time: 499 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd, numpy as np, re\n",
    "from datetime import datetime\n",
    "\n",
    "IN_PATH  = \"join/ico_union_wide.csv\"\n",
    "OUT_PATH = \"final/ico_union_canonical.csv\"\n",
    "\n",
    "# --- Load ---\n",
    "df = pd.read_csv(IN_PATH)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# --- Suffix config (prioridad de fuentes) ---\n",
    "SUFFIXES = [\"_zenodo\", \"_icpsr\", \"_yan\"]  # prioridad por orden\n",
    "\n",
    "# --- Utilities ---\n",
    "def split_base_and_suffix(col: str):\n",
    "    for s in SUFFIXES:\n",
    "        if col.endswith(s):\n",
    "            return col[:-len(s)], s\n",
    "    return col, \"\"  # columnas sin sufijo\n",
    "\n",
    "# agrupar columnas por \"base\"\n",
    "base_to_cols = {}\n",
    "for c in df.columns:\n",
    "    b, s = split_base_and_suffix(c)\n",
    "    base_to_cols.setdefault(b, []).append(c)\n",
    "\n",
    "def _first_nonnull(series_list):\n",
    "    \"\"\"coalesce: primera serie con dato no nulo por fila.\"\"\"\n",
    "    if not series_list:\n",
    "        return pd.Series([np.nan]*len(df))\n",
    "    out = pd.Series([np.nan]*len(df))\n",
    "    for s in series_list:\n",
    "        if s is None: \n",
    "            continue\n",
    "        if isinstance(s, str) and s in df.columns:\n",
    "            v = df[s]\n",
    "        elif isinstance(s, pd.Series):\n",
    "            v = s\n",
    "        else:\n",
    "            continue\n",
    "        out = out.where(~out.isna(), v)\n",
    "    return out\n",
    "\n",
    "def choose_cols_by_priority(base_name, prefer_numeric=False, regex=False):\n",
    "    \"\"\"\n",
    "    Devuelve lista de columnas (nombres) para un base_name, ordenadas por prioridad de dataset.\n",
    "    - Si regex=True, base_name es un patrón y trae todas las bases que 'matchean'.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    # matching de base exacto o por regex\n",
    "    bases = []\n",
    "    if regex:\n",
    "        pat = re.compile(base_name, flags=re.IGNORECASE)\n",
    "        bases = [b for b in base_to_cols.keys() if pat.search(b)]\n",
    "    else:\n",
    "        if base_name in base_to_cols:\n",
    "            bases = [base_name]\n",
    "        else:\n",
    "            # fallback: probar case-insensitive\n",
    "            for b in base_to_cols:\n",
    "                if b.lower() == base_name.lower():\n",
    "                    bases = [b]; break\n",
    "    # por cada base, ordenar por prioridad de sufijo\n",
    "    for b in bases:\n",
    "        cols = base_to_cols[b]\n",
    "        # separar con sufijo y sin sufijo\n",
    "        with_suf = [c for c in cols if any(c.endswith(s) for s in SUFFIXES)]\n",
    "        no_suf   = [c for c in cols if c not in with_suf]\n",
    "        # ordenar los con sufijo por prioridad\n",
    "        ordered = []\n",
    "        for s in SUFFIXES:\n",
    "            ordered += [c for c in with_suf if c.endswith(s)]\n",
    "        ordered += no_suf  # al final, sin sufijo\n",
    "        # ordenar numéricos antes si se pide\n",
    "        if prefer_numeric:\n",
    "            numeric_first = [c for c in ordered if pd.api.types.is_numeric_dtype(df[c])]\n",
    "            non_numeric   = [c for c in ordered if not pd.api.types.is_numeric_dtype(df[c])]\n",
    "            ordered = numeric_first + non_numeric\n",
    "        candidates += ordered\n",
    "    return candidates\n",
    "\n",
    "def parse_money_like(s):\n",
    "    \"\"\"intenta homogenizar strings de montos a float.\"\"\"\n",
    "    if pd.isna(s): return np.nan\n",
    "    x = str(s).strip().lower().replace(\",\", \"\").replace(\"$\", \"\")\n",
    "    try:\n",
    "        mult = 1\n",
    "        if \"billion\" in x or (re.search(r\"\\d\", x) and x.endswith(\"b\")): mult = 1_000_000_000\n",
    "        elif \"million\" in x or (re.search(r\"\\d\", x) and x.endswith(\"m\")): mult = 1_000_000\n",
    "        elif re.search(r\"\\d\", x) and x.endswith(\"k\"): mult = 1_000\n",
    "        nums = re.findall(r\"[\\d.]+\", x)\n",
    "        return float(nums[0]) * mult if nums else np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def clean_date_like(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    x = str(s).strip()\n",
    "    x = re.sub(r\"^(ended|end|finished|finalized)\\s*:?\\s*\", \"\", x, flags=re.IGNORECASE)\n",
    "    x = x.replace(\"—\",\"-\").replace(\"–\",\"-\")\n",
    "    return x\n",
    "\n",
    "def parse_date_series(series, formats=(\"%d %b %Y\", \"%Y-%m-%d\", \"%d/%m/%Y\", \"%b %d, %Y\")):\n",
    "    \"\"\"intenta parsear fechas a Timestamp, probando varios formatos.\"\"\"\n",
    "    out = pd.to_datetime(series, errors=\"coerce\", dayfirst=True)\n",
    "    # si sigue muy NaT y hay strings, intentar otros formatos\n",
    "    if out.isna().any():\n",
    "        s = series.astype(str)\n",
    "        for fmt in formats:\n",
    "            mask = out.isna()\n",
    "            try:\n",
    "                out.loc[mask] = pd.to_datetime(s[mask], format=fmt, errors=\"coerce\", dayfirst=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return out\n",
    "\n",
    "def boolify(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    if isinstance(x, (int, float)) and not pd.isna(x):\n",
    "        return int(float(x) != 0)\n",
    "    s = str(x).strip().lower()\n",
    "    if s in {\"1\",\"true\",\"yes\",\"y\",\"si\",\"sí\"}: return 1\n",
    "    if s in {\"0\",\"false\",\"no\",\"n\"}: return 0\n",
    "    return np.nan\n",
    "\n",
    "def extract_min_max(s):\n",
    "    \"\"\"parsea ranges tipo '0.1-10 ETH' -> (min, max) (en unidades sin conversión de divisa).\"\"\"\n",
    "    if pd.isna(s): return (np.nan, np.nan)\n",
    "    x = str(s).lower()\n",
    "    nums = re.findall(r\"[\\d.]+\", x)\n",
    "    if not nums: return (np.nan, np.nan)\n",
    "    if len(nums) == 1:\n",
    "        v = float(nums[0]); return (v, v)\n",
    "    return (float(nums[0]), float(nums[1]))\n",
    "\n",
    "# --- Canonical schema (muy amplio) ---\n",
    "CANON = {\n",
    "    # Identidad\n",
    "    \"name_std\":           [[\"name_std\"]],\n",
    "    \"symbol_std\":         [[\"symbol_std\"]],\n",
    "\n",
    "    # Fechas (start/end)\n",
    "    \"ico_start_date\":     [[\"start_date\",\"start\",\"ico_start\",\"sale_start\",\"preico_start\",\"token_sale_start\"],\n",
    "                           [r\"start_end_date_coin_sell\", r\"date_range\", r\"ico_dates\"]],  # tomaremos el extremo izquierdo si viene rango\n",
    "    \"ico_end_date\":       [[\"end_date_parsed\",\"end_date\",\"end\",\"ico_end\",\"sale_end\",\"token_sale_end\"],\n",
    "                           [r\"start_end_date_coin_sell\", r\"date_range\", r\"ico_dates\"]],  # tomaremos el extremo derecho si viene rango\n",
    "\n",
    "    # Recaudación y objetivos\n",
    "    \"goal_usd\":           [[\"fundraising_goal\",\"goal\",\"soft_cap\",\"softcap\",\"target\"]],\n",
    "    \"hard_cap_usd\":       [[\"hard_cap\",\"hardcap\",\"max_cap\",\"maximum_cap\"]],\n",
    "    \"amount_raised_usd\":  [[\"received_money\",\"amount_raised\",\"raised\",\"raised_usd\",\"received_money.1\"]],  # variantes\n",
    "    \"ico_successful\":     [[\"ico_successful\",\"success\",\"successful\"]],\n",
    "\n",
    "    # Tokenomics\n",
    "    \"token_price_usd\":    [[\"ico_token_price\",\"token_price\"]],\n",
    "    \"total_tokens\":       [[\"total_tokens\",\"supply_total\",\"token_supply_total\"]],\n",
    "    \"tokens_for_sale\":    [[\"available_for_token_sale\",\"token_sale_amount\",\"for_sale\"]],\n",
    "    \"min_investment_raw\": [[\"min_investment\",\"min_max_personal_cap\",\"minimum_investment\"]],\n",
    "    \"max_investment_raw\": [[\"max_investment\",\"min_max_personal_cap\",\"maximum_investment\"]],\n",
    "    \"token_type\":         [[\"token_type\",\"type\"]],\n",
    "    \"role_of_token\":      [[\"role_of_token\",\"role\"]],\n",
    "\n",
    "    # Acceso / Compliance / Jurisdicción\n",
    "    \"whitelist\":          [[\"whitelist\"]],\n",
    "    \"kyc\":                [[\"kyc\"]],\n",
    "    \"jurisdiction\":       [[\"jurisdiction\",\"country\"]],\n",
    "    \"accepts\":            [[\"accepts\",\"currencies_accepted\"]],\n",
    "\n",
    "    # Señales de ejecución / presencia\n",
    "    \"has_github\":         [[\"has_github\",\"github\",\"github_available\"]],\n",
    "    \"has_telegram\":       [[\"has_telegram\",\"telegram\"]],\n",
    "    \"has_reddit\":         [[\"has_reddit\",\"reddit\"]],\n",
    "    \"website_available\":  [[\"website_available\",\"website\",\"site\"]],\n",
    "\n",
    "    # Equipo / rating / interés / docs\n",
    "    \"team_size\":          [[\"team_size\",\"teamsize\"]],\n",
    "    \"rating\":             [[\"rating\",\"score\",\"ico_rating\"]],\n",
    "    \"interest\":           [[\"interest\"]],\n",
    "    \"discount_max_pct\":   [[\"crowdsale max. discount\",\"max_discount\",\"discount\"]],\n",
    "    \"roadmap_available\":  [[\"development road map available\",\"roadmap_available\",\"has_roadmap\"]],\n",
    "    \"whitepaper_available\":[[\"whitepaper_available\",\"whitepaper\",\"has_whitepaper\"]],\n",
    "}\n",
    "\n",
    "# --- Resolver columnas por base + prioridad fuente ---\n",
    "def resolve_base_to_series(base_tokens):\n",
    "    \"\"\"\n",
    "    base_tokens: lista de bases (strings exactos) o patrones regex (si empiezan con '^' o contienen '.*')\n",
    "    Devuelve lista de columnas reales en orden de prioridad de fuente.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    for bt in base_tokens:\n",
    "        if re.search(r\"[\\^\\$\\.\\*\\+\\|\\(\\)\\[\\]\\?]\", bt, flags=re.I):  # patrón regex\n",
    "            cols += choose_cols_by_priority(bt, regex=True)\n",
    "        else:\n",
    "            cols += choose_cols_by_priority(bt, regex=False)\n",
    "    # quitar duplicados manteniendo orden\n",
    "    seen = set(); cols_unique = []\n",
    "    for c in cols:\n",
    "        if c not in seen and c in df.columns:\n",
    "            seen.add(c); cols_unique.append(c)\n",
    "    return cols_unique\n",
    "\n",
    "# --- Construcción del DataFrame canónico ---\n",
    "out = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Identidad directa\n",
    "for canon, groups in CANON.items():\n",
    "    if canon in [\"name_std\",\"symbol_std\"]:\n",
    "        cols = resolve_base_to_series(groups[0])\n",
    "        out[canon] = _first_nonnull(cols)\n",
    "    else:\n",
    "        out[canon] = np.nan  # inicializamos\n",
    "\n",
    "# Fechas: start/end con soporte de rango\n",
    "def pick_date_left_right():\n",
    "    # Start: primero intentamos columnas de inicio directas\n",
    "    start_cols = resolve_base_to_series(CANON[\"ico_start_date\"][0])\n",
    "    start_series = _first_nonnull(start_cols)\n",
    "    # Si todo NaT, intentamos rango (tomar izquierda del rango)\n",
    "    if start_series.isna().all() and len(CANON[\"ico_start_date\"])>1:\n",
    "        rng_cols = resolve_base_to_series(CANON[\"ico_start_date\"][1])\n",
    "        if rng_cols:\n",
    "            left = df[rng_cols[0]].astype(str).str.extract(r\"^\\s*([^-–—|to]+)\", expand=False).map(clean_date_like)\n",
    "            start_series = left\n",
    "    start_series = parse_date_series(start_series)\n",
    "\n",
    "    # End: primero columnas de fin directas\n",
    "    end_cols = resolve_base_to_series(CANON[\"ico_end_date\"][0])\n",
    "    end_series = _first_nonnull(end_cols)\n",
    "    # si NaT, intentar derecha del rango\n",
    "    if end_series.isna().all() and len(CANON[\"ico_end_date\"])>1:\n",
    "        rng_cols = resolve_base_to_series(CANON[\"ico_end_date\"][1])\n",
    "        if rng_cols:\n",
    "            right = df[rng_cols[0]].astype(str).str.extract(r\"[-–—|to]\\s*(.*)$\", expand=False).map(clean_date_like)\n",
    "            end_series = right\n",
    "    end_series = parse_date_series(end_series)\n",
    "    return start_series, end_series\n",
    "\n",
    "out[\"ico_start_date\"], out[\"ico_end_date\"] = pick_date_left_right()\n",
    "\n",
    "# Numéricos principales (coalesce con preferencia de columnas numéricas)\n",
    "def coalesce_numeric(cand_groups):\n",
    "    cols = []\n",
    "    for g in cand_groups:\n",
    "        cols += resolve_base_to_series(g)\n",
    "    # ordenar numeric dtype primero por cada grupo de prioridad\n",
    "    numeric_first = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    others = [c for c in cols if c not in numeric_first]\n",
    "    cols_ordered = numeric_first + others\n",
    "    ser = _first_nonnull(cols_ordered)\n",
    "    # intentar parseo para strings tipo \"3M\"\n",
    "    ser = ser.apply(parse_money_like)\n",
    "    return ser\n",
    "\n",
    "out[\"goal_usd\"]          = coalesce_numeric(CANON[\"goal_usd\"])\n",
    "out[\"hard_cap_usd\"]      = coalesce_numeric(CANON[\"hard_cap_usd\"])\n",
    "out[\"amount_raised_usd\"] = coalesce_numeric(CANON[\"amount_raised_usd\"])\n",
    "\n",
    "# Etiqueta de éxito: si hay varias, priorizamos por fuentes (implícito en union_wide)\n",
    "succ_cols = resolve_base_to_series(CANON[\"ico_successful\"][0])\n",
    "succ_series = _first_nonnull(succ_cols)\n",
    "out[\"ico_successful\"] = succ_series.map(boolify)\n",
    "\n",
    "# Tokenomics\n",
    "out[\"token_price_usd\"] = coalesce_numeric(CANON[\"token_price_usd\"])\n",
    "out[\"total_tokens\"]    = coalesce_numeric(CANON[\"total_tokens\"])\n",
    "out[\"tokens_for_sale\"] = coalesce_numeric(CANON[\"tokens_for_sale\"])\n",
    "\n",
    "# Min/Max investment: si no hay columnas directas, intentar parsear min_max_personal_cap\n",
    "min_cols = resolve_base_to_series(CANON[\"min_investment_raw\"][0])\n",
    "max_cols = resolve_base_to_series(CANON[\"max_investment_raw\"][0])\n",
    "\n",
    "if not min_cols and not max_cols:\n",
    "    # buscar cualquier base que contenga 'min_max_personal_cap'\n",
    "    mm = choose_cols_by_priority(r\"min[_\\- ]?max[_\\- ]?personal[_\\- ]?cap\", regex=True)\n",
    "    if mm:\n",
    "        mn, mx = zip(*df[mm[0]].map(extract_min_max))\n",
    "        out[\"min_investment_usd\"] = pd.to_numeric(mn, errors=\"coerce\")\n",
    "        out[\"max_investment_usd\"] = pd.to_numeric(mx, errors=\"coerce\")\n",
    "else:\n",
    "    if min_cols:\n",
    "        out[\"min_investment_usd\"] = coalesce_numeric([min_cols])\n",
    "    if max_cols:\n",
    "        out[\"max_investment_usd\"] = coalesce_numeric([max_cols])\n",
    "\n",
    "# Categóricas / flags\n",
    "for canon in [\"token_type\",\"role_of_token\",\"jurisdiction\",\"accepts\",\"interest\",\"rating\"]:\n",
    "    cols = []\n",
    "    for g in CANON[canon]:\n",
    "        cols += resolve_base_to_series(g)\n",
    "    out[canon] = _first_nonnull(cols)\n",
    "\n",
    "for canon in [\"whitelist\",\"kyc\",\"has_github\",\"has_telegram\",\"has_reddit\",\"website_available\",\n",
    "              \"roadmap_available\",\"whitepaper_available\"]:\n",
    "    cols = []\n",
    "    for g in CANON[canon]:\n",
    "        cols += resolve_base_to_series(g)\n",
    "    out[canon] = _first_nonnull(cols).map(boolify)\n",
    "\n",
    "# Descuento crowd-sale\n",
    "disc_cols = []\n",
    "for g in CANON[\"discount_max_pct\"]:\n",
    "    disc_cols += resolve_base_to_series(g)\n",
    "disc = _first_nonnull(disc_cols)\n",
    "disc = disc.astype(str).str.extract(r\"([\\d.]+)\", expand=False)\n",
    "out[\"discount_max_pct\"] = pd.to_numeric(disc, errors=\"coerce\")\n",
    "\n",
    "# Reglas derivadas\n",
    "out[\"hit_softcap\"] = ((out[\"amount_raised_usd\"] >= out[\"goal_usd\"]) & out[\"amount_raised_usd\"].notna() & out[\"goal_usd\"].notna()).astype(\"Int64\")\n",
    "out[\"hit_hardcap\"] = ((out[\"amount_raised_usd\"] >= out[\"hard_cap_usd\"]) & out[\"amount_raised_usd\"].notna() & out[\"hard_cap_usd\"].notna()).astype(\"Int64\")\n",
    "\n",
    "# Orden de columnas final (identidad -> fechas -> funding -> tokenomics -> acceso -> señales -> equipo/rating/docs)\n",
    "ordered_cols = [\n",
    "    \"name_std\",\"symbol_std\",\n",
    "    \"ico_start_date\",\"ico_end_date\",\n",
    "    \"goal_usd\",\"hard_cap_usd\",\"amount_raised_usd\",\"ico_successful\",\"hit_softcap\",\"hit_hardcap\",\n",
    "    \"token_price_usd\",\"total_tokens\",\"tokens_for_sale\",\"min_investment_usd\",\"max_investment_usd\",\n",
    "    \"token_type\",\"role_of_token\",\"whitelist\",\"kyc\",\"jurisdiction\",\"accepts\",\n",
    "    \"has_github\",\"has_telegram\",\"has_reddit\",\"website_available\",\n",
    "    \"team_size\",\"rating\",\"interest\",\"discount_max_pct\",\"roadmap_available\",\"whitepaper_available\",\n",
    "]\n",
    "# añade cualquier columna canónica que haya quedado fuera por no existir\n",
    "ordered_cols = [c for c in ordered_cols if c in out.columns] + [c for c in out.columns if c not in ordered_cols]\n",
    "\n",
    "out = out[ordered_cols].copy()\n",
    "\n",
    "# --- Diagnostics ---\n",
    "def missing_pct(s): \n",
    "    return round(100*s.isna().mean(), 2)\n",
    "\n",
    "report = pd.DataFrame({\n",
    "    \"column\": out.columns,\n",
    "    \"dtype\": [str(out[c].dtype) for c in out.columns],\n",
    "    \"missing_%\": [missing_pct(out[c]) for c in out.columns]\n",
    "}).sort_values([\"missing_%\",\"column\"], ascending=[False, True])\n",
    "\n",
    "print(f\"Filas: {len(out):,}  |  Columnas canónicas: {out.shape[1]}\")\n",
    "print(\"\\nTop 20 columnas con más missing (%):\")\n",
    "display(report.head(20))\n",
    "\n",
    "# --- Guardar ---\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\n✅ Guardado canónico: {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9e7fa-b310-4af6-be66-abb88066df18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
