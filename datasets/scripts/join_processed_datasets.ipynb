{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0a7c2b2-044e-4bf5-8f74-35098e15cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "# Rutas de entrada (ajustá si hace falta)\n",
    "BASE = Path(\"../processed\")\n",
    "PATHS = {\n",
    "    \"zenodo\":  BASE / \"zenodo_fahlenbrach_clean.csv\",\n",
    "    \"icpsr\":   BASE / \"icpsr_villanueva_clean.csv\",\n",
    "    \"yan\":     BASE / \"kaggle_yanmaksi_clean.csv\",\n",
    "}\n",
    "\n",
    "# Rutas de salida\n",
    "OUT_DIR = Path(\"../join\")\n",
    "OUT_UNION = OUT_DIR / \"ico_union_wide.csv\"\n",
    "OUT_MULTI = OUT_DIR / \"present_in_multiple.csv\"\n",
    "OUT_COLL  = OUT_DIR / \"value_collisions.csv\"\n",
    "\n",
    "def normalize_text(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.lower()\n",
    "         .str.strip()\n",
    "         .str.replace(r\"[^a-z0-9]\", \"\", regex=True)\n",
    "         .replace({\"nan\": \"\"})\n",
    "    )\n",
    "\n",
    "def ensure_keys(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Asegura name_std y symbol_std si faltan, usando columnas alternativas.\"\"\"\n",
    "    cols = df.columns.str.lower()\n",
    "    df.columns = cols\n",
    "\n",
    "    if \"name_std\" not in df.columns:\n",
    "        name_col = next((c for c in [\"name\",\"project name\",\"project_name\",\"ico_name\",\"token\"] if c in df.columns), None)\n",
    "        df[\"name_std\"] = normalize_text(df[name_col]) if name_col else \"\"\n",
    "\n",
    "    if \"symbol_std\" not in df.columns:\n",
    "        sym_col = next((c for c in [\"symbol\",\"ticker\",\"ticker_symbol\",\"coin_ticker\",\"short\",\"abbr\"] if c in df.columns), None)\n",
    "        df[\"symbol_std\"] = normalize_text(df[sym_col]) if sym_col else \"\"\n",
    "\n",
    "    # quitar whitespace/NaN residuales\n",
    "    df[\"name_std\"]   = df[\"name_std\"].fillna(\"\").astype(str)\n",
    "    df[\"symbol_std\"] = df[\"symbol_std\"].fillna(\"\").astype(str)\n",
    "    return df\n",
    "\n",
    "def load_tagged(path: Path, tag: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df = ensure_keys(df)\n",
    "    df[\"__source__\"] = tag\n",
    "    return df\n",
    "\n",
    "def make_key(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Key primaria (name_std + symbol_std) con fallback si uno está vacío.\"\"\"\n",
    "    k = df[\"name_std\"].fillna(\"\") + \"||\" + df[\"symbol_std\"].fillna(\"\")\n",
    "    # fallback: si ambos vacíos, usar índice para no perder filas (igual quedarán como no unibles)\n",
    "    empty_mask = (df[\"name_std\"] == \"\") & (df[\"symbol_std\"] == \"\")\n",
    "    if empty_mask.any():\n",
    "        k = k.where(~empty_mask, \"__row__:\"+df.reset_index().index.astype(str))\n",
    "    return k\n",
    "\n",
    "def friendly_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Evita columnas duplicadas exactas (mismo nombre) antes del merge\n",
    "    return df.loc[:, ~df.columns.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bb5efc2-5388-40b7-8b10-768cdad00a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zenodo (306, 132) keys vacíos: 0 0\n",
      "icpsr (2186, 79) keys vacíos: 0 15\n",
      "yan (200, 28) keys vacíos: 0 29\n",
      "✅ Guardado union wide: ..\\join\\ico_union_wide.csv — filas: 2,692, cols: 240\n"
     ]
    }
   ],
   "source": [
    "### Chequeo rápido\n",
    "dfs = {tag: load_tagged(path, tag) for tag, path in PATHS.items()}\n",
    "\n",
    "for tag, d in dfs.items():\n",
    "    print(tag, d.shape, \"keys vacíos:\", (d[\"name_std\"]==\"\" ).sum(), (d[\"symbol_std\"]==\"\").sum())\n",
    "\n",
    "# Clave de unión por dataset\n",
    "for tag in dfs:\n",
    "    dfs[tag][\"__key__\"] = make_key(dfs[tag])\n",
    "    dfs[tag] = friendly_cols(dfs[tag])\n",
    "\n",
    "# Outer merge de los tres con sufijos distintos\n",
    "order = [\"zenodo\",\"icpsr\",\"yan\"]\n",
    "suf_map = {\"zenodo\":\"_zenodo\",\"icpsr\":\"_icpsr\",\"yan\":\"_yan\"}\n",
    "\n",
    "# Arrancamos merge con el primero\n",
    "merged = dfs[order[0]].copy()\n",
    "\n",
    "for nxt in order[1:]:\n",
    "    merged = merged.merge(\n",
    "        dfs[nxt],\n",
    "        how=\"outer\",\n",
    "        on=\"__key__\",\n",
    "        suffixes=(\"\", suf_map[nxt])  # primer choque usa sufijo en el RHS\n",
    "    )\n",
    "    # Para evitar conflictos en siguientes merges, renombramos colisiones genéricas\n",
    "    # (ya que pandas sólo aplica suffixes al segundo DF de cada merge)\n",
    "    dup_cols = merged.columns[merged.columns.duplicated()].unique()\n",
    "    if len(dup_cols) > 0:\n",
    "        for c in dup_cols:\n",
    "            # renombrar la col duplicada más a la derecha con sufijo del dataset\n",
    "            cols = [i for i, name in enumerate(merged.columns) if name == c]\n",
    "            # dejamos la primera tal cual, renombramos el resto con sufijo del dataset reciente\n",
    "            for idxpos in cols[1:]:\n",
    "                merged.columns.values[idxpos] = f\"{c}{suf_map[nxt]}\"\n",
    "\n",
    "# Reordenar: keys y un pequeño set de columnas “clave” al frente si existen\n",
    "front = [c for c in [\"__key__\",\"name_std\",\"symbol_std\"] if c in merged.columns]\n",
    "other = [c for c in merged.columns if c not in front]\n",
    "merged = merged[front + other]\n",
    "\n",
    "# Guardar unión “wide” (todas las features con sufijos)\n",
    "merged.to_csv(OUT_UNION, index=False)\n",
    "print(f\"✅ Guardado union wide: {OUT_UNION} — filas: {len(merged):,}, cols: {merged.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e796741d-15e0-4b01-9cf1-18a51b3b9e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Guardado tokens en >1 dataset: ..\\join\\present_in_multiple.csv — filas: 0\n",
      "✅ Guardado colisiones de valores: ..\\join\\value_collisions.csv — filas: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__key__</th>\n",
       "      <th>name_std</th>\n",
       "      <th>symbol_std</th>\n",
       "      <th>present_zenodo</th>\n",
       "      <th>present_icpsr</th>\n",
       "      <th>present_yan</th>\n",
       "      <th>in_n_datasets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [__key__, name_std, symbol_std, present_zenodo, present_icpsr, present_yan, in_n_datasets]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__key__</th>\n",
       "      <th>name_std</th>\n",
       "      <th>symbol_std</th>\n",
       "      <th>feature_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [__key__, name_std, symbol_std, feature_base]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Celda 3 (patched) — Tokens en >1 dataset + Colisiones de valores ===\n",
    "# Fix mínimo: asegurar __key__ en dfs[*] y fallback si no existe 'merged'\n",
    "\n",
    "# 0) Asegurar que 'dfs' exista (si no corriste la celda 2)\n",
    "if 'dfs' not in globals():\n",
    "    dfs = {tag: load_tagged(path, tag) for tag, path in PATHS.items()}  # usa helpers de Celda 1\n",
    "\n",
    "# 1) Asegurar que cada df tenga __key__\n",
    "for tag in list(dfs.keys()):\n",
    "    d = dfs[tag]\n",
    "    # normalizar claves mínimas si faltan\n",
    "    if \"name_std\" not in d.columns or \"symbol_std\" not in d.columns:\n",
    "        d = ensure_keys(d)\n",
    "    if \"__key__\" not in d.columns:\n",
    "        d[\"__key__\"] = make_key(d)\n",
    "    dfs[tag] = d\n",
    "\n",
    "# 2) Intentar tener 'merged' disponible\n",
    "minimal_merged = False\n",
    "if 'merged' not in globals():\n",
    "    try:\n",
    "        # intentar cargar el union wide previamente guardado\n",
    "        merged = pd.read_csv(OUT_UNION)\n",
    "        if \"__key__\" not in merged.columns:\n",
    "            # si el union no trae __key__, lo construimos desde name_std+symbol_std\n",
    "            if \"name_std\" in merged.columns and \"symbol_std\" in merged.columns:\n",
    "                merged[\"__key__\"] = merged[\"name_std\"].fillna(\"\") + \"||\" + merged[\"symbol_std\"].fillna(\"\")\n",
    "            else:\n",
    "                minimal_merged = True\n",
    "    except Exception:\n",
    "        minimal_merged = True\n",
    "\n",
    "# 2.b) Si no pudimos cargar un union wide, armamos un merged mínimo\n",
    "if minimal_merged:\n",
    "    # Unión de todas las keys y coalesce de name_std/symbol_std desde los dfs (prioridad zenodo>icpsr>yan)\n",
    "    key_union = sorted(set().union(*[set(dfs[t][\"__key__\"]) for t in dfs]))\n",
    "    merged = pd.DataFrame({\"__key__\": key_union})\n",
    "    for col in [\"name_std\", \"symbol_std\"]:\n",
    "        # coalesce por prioridad\n",
    "        val = pd.Series(\"\", index=merged.index)\n",
    "        for t in [\"zenodo\",\"icpsr\",\"yan\"]:\n",
    "            if col in dfs[t].columns:\n",
    "                m = merged[[\"__key__\"]].merge(dfs[t][[\"__key__\", col]], on=\"__key__\", how=\"left\")[col].fillna(\"\")\n",
    "                val = np.where(val == \"\", m, val)\n",
    "        merged[col] = val\n",
    "\n",
    "# ----- (A) tokens presentes en más de 1 dataset -----\n",
    "presence_cols = []\n",
    "for tag in [\"zenodo\",\"icpsr\",\"yan\"]:\n",
    "    col = f\"present_{tag}\"\n",
    "    presence_cols.append(col)\n",
    "\n",
    "# sets de keys originales por dataset\n",
    "keys_sets = {tag: set(dfs[tag][\"__key__\"].tolist()) for tag in dfs}\n",
    "\n",
    "# dataframe base para presencia\n",
    "base_cols = [c for c in [\"__key__\", \"name_std\", \"symbol_std\"] if c in merged.columns]\n",
    "present_df = merged[base_cols].copy()\n",
    "\n",
    "for tag in [\"zenodo\",\"icpsr\",\"yan\"]:\n",
    "    present_df[f\"present_{tag}\"] = present_df[\"__key__\"].isin(keys_sets[tag]).astype(int)\n",
    "\n",
    "present_df[\"in_n_datasets\"] = present_df[presence_cols].sum(axis=1)\n",
    "multi = present_df[present_df[\"in_n_datasets\"] >= 2].copy()\n",
    "multi.to_csv(OUT_MULTI, index=False)\n",
    "print(f\"✅ Guardado tokens en >1 dataset: {OUT_MULTI} — filas: {len(multi):,}\")\n",
    "\n",
    "# ----- (B) colisiones de valores (misma feature con valores distintos) -----\n",
    "# Si no tenemos union wide (con sufijos), evitamos falso positivo y generamos vacío controlado\n",
    "suffixes = [\"_zenodo\",\"_icpsr\",\"_yan\"]\n",
    "has_wide = any(col.endswith(tuple(suffixes)) for col in merged.columns)\n",
    "\n",
    "if not has_wide:\n",
    "    coll_df = pd.DataFrame(columns=[\"__key__\",\"name_std\",\"symbol_std\",\"feature_base\"])\n",
    "    coll_df.to_csv(OUT_COLL, index=False)\n",
    "    print(f\"ℹ️ No hay columnas con sufijos *_zenodo/_icpsr/_yan en 'merged'; \"\n",
    "          f\"se guarda {OUT_COLL} vacío (corré la Celda 2 para generar el union wide).\")\n",
    "else:\n",
    "    cols = merged.columns\n",
    "\n",
    "    def base_name(c):\n",
    "        for s in suffixes:\n",
    "            if c.endswith(s):\n",
    "                return c[: -len(s)]\n",
    "        return None\n",
    "\n",
    "    from collections import defaultdict\n",
    "    by_base = defaultdict(list)\n",
    "    for c in cols:\n",
    "        b = base_name(c)\n",
    "        if b:\n",
    "            by_base[b].append(c)\n",
    "\n",
    "    collisions = []\n",
    "    for b, cols_same in by_base.items():\n",
    "        if len(cols_same) < 2:\n",
    "            continue\n",
    "        sub_cols = [c for c in [\"__key__\",\"name_std\",\"symbol_std\"] if c in merged.columns] + cols_same\n",
    "        sub = merged[sub_cols].copy()\n",
    "\n",
    "        def norm_series(s):\n",
    "            if pd.api.types.is_numeric_dtype(s):\n",
    "                return pd.to_numeric(s, errors=\"coerce\").round(8).astype(str)\n",
    "            else:\n",
    "                return s.astype(str).str.strip()\n",
    "\n",
    "        for c in cols_same:\n",
    "            sub[c] = norm_series(sub[c])\n",
    "\n",
    "        nonnull_counts = sub[cols_same].replace({\"\": np.nan, \"nan\": np.nan}).notna().sum(axis=1)\n",
    "        unique_counts = sub[cols_same].apply(lambda r: len(set([x for x in r if x not in [\"\", \"nan\"]])), axis=1)\n",
    "        mask = (nonnull_counts >= 2) & (unique_counts >= 2)\n",
    "\n",
    "        if mask.any():\n",
    "            tmp = sub.loc[mask].copy()\n",
    "            if \"feature_base\" not in tmp.columns:\n",
    "                tmp.insert(len(sub.columns)-len(cols_same), \"feature_base\", b)\n",
    "            collisions.append(tmp)\n",
    "\n",
    "    coll_df = pd.concat(collisions, axis=0, ignore_index=True) if collisions else \\\n",
    "              pd.DataFrame(columns=[\"__key__\",\"name_std\",\"symbol_std\",\"feature_base\"])\n",
    "    coll_df.to_csv(OUT_COLL, index=False)\n",
    "    print(f\"✅ Guardado colisiones de valores: {OUT_COLL} — filas: {len(coll_df):,}\")\n",
    "\n",
    "# Vista rápida\n",
    "display(multi.head(10))\n",
    "if 'coll_df' in locals():\n",
    "    display(coll_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e551af0-9515-495a-b7e5-9cd5142a7868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zenodo: ['name_other', 'name_cmc', 'ticker_symbol_cmc', 'ico_successful', 'soft_cap', 'hard_cap', 'cap_unit', 'cap_includes_presale', 'token_type', 'number_of_contributors', 'crowdsale_tokens_sold', 'total_number_of_tokens', 'token_standard', 'additional_token_emissions', 'crowdsale_token_price_min', 'crowdsale_token_price_max', 'crowdsale_actual_token_price_max', 'crowdsale is auction', 'has a presale', 'presale_tokens_sold', 'presale_token_price_min', 'presale_token_price_max', 'development road map available', 'whitepaper page count', 'product or prototype developed', 'product can be tried out', 'years since foundation', 'issuer has customers for product', 'business model available', 'utility token enables decentralization', 'smart contract code available', 'project code available', 'use of proceeds mentioned', 'use of proceeds disclosed in detail', 'token share team (ex ante)', 'token share crowdsale investors (ex ante)', 'token share presale investors (ex ante)', 'token share producers/miners (ex ante)', \"unsold tokens 'burnt' or proportional allocation\", 'unsold tokens kept by issuer', 'unsold tokens locked up', 'lock up period unsold tokens (years)', 'team lockup period (weighted avg.)', 'presale lockup period (weighted avg.)', 'vc_support_general', 'vc_support_blockchain_specialist', 'investors have governance rights', 'team size', 'experienced team', 'team member with business background', 'advisor_quality', 'legal_structure', 'registration_country', 'address', 'independent custodian for ico funds', 'portfolio_converted_into_fiat', 'funding milestones', 'qualified investors only', 'simple agreement for future tokens (saft)', 'kyc/aml procedure', 'us retail investors excluded', 'investors from other (non-us) jurisdictions excluded', 'legal advisor disclosed', 'financial advisor disclosed', 'air_drop_after_ico', 'industry', 'ethereum_contract_address', 'ico_start_date', 'ico_end_date_planned', 'ico_end_date_actual', 'amount raised in presale (usdm)', 'amount raised in crowdsale (usdm)', 'total amount raised (usdm)', 'total amount raised (usdm).1', 'btc_ret_ico_period', 'presale discount (%)', \"fundraiser has maximum ('hard cap')\", \"fundraiser has minimum ('soft cap')\", 'soft_cap_unit', 'hard_cap_unit', 'percentage of hard cap raised (%)', 'pct_of_crowdsale_target_raised', 'price_presale_avg', 'price_presale', 'price_crowdsale_avg', 'price_crowdsale', 'crowdsale max. discount (%)', 'us_qualified_only', 'team business background missing', 'team experience missing', 'average_discount', 'team tokens locked up', 'presale tokens locked up', 'issued_on_other_platf', 'legal form and jurisdiction known', 'postal address known', 'registered in offshore financial center', 'switzerland', 'is_ethereum', 'is cryptographic token', 'has vc backing', 'has_generalist_vc', 'has_specialist_vc', 'unknown or low quality advisors', 'high quality advisory team', 'token supply is fixed', 'token share crowdsale investors (ex post)', 'post_money_val_crowdsale', 'celebrity endorsement', 'is a security', 'is a utility token', 'token_is_cryptocurrency', 'token_is_new_blockchain', 'is currency or general purpose blockchain', 'decentralised platform', 'legal entity is foundation', 'legal entity is corporation', 'legal entity is llc', 'legal entity is corporation or llc', 'length of crowdsale (calendar days, actual)', 'length of crowdsale (calendar days, planned)', 'lock_up_period_team_ep', 'presale_transparent', 'industry.1', '(first) date', 'time to listing (calendar days)', 'name_std', 'symbol_std', 'soft_cap_usd', 'hard_cap_usd', 'independent custodian for ico funds_usd', '__source__', '__key__']\n",
      "\n",
      "ICPSR: ['web adress', 'name', 'ticker', 'ico success', 'industry', 'platform', 'ieo', 'mininvest', 'country', 'kyc', 'wlist', 'regulkyc', 'regtax', 'restusa', 'restchina', 'tokens f sale', 'accepting', 'eth', 'btc', 'fiat', 'ltc', 'dash', 'xrp', 'zec', 'neo', 'eos', 'other', 'protocol type', 'distributed in ico', 'softcap', 'hardcap', 'amount raised', 'circsupply', 'price mkt', 'price pre ico', 'price ico', 'regulkyc dates', 'pre ico starts', 'pre ico ends', 'total days', 'ico starts', 'ico ends', 'total days.1', 'artificial intelligence', 'art', 'banking', 'big data', 'business services', 'charity', 'communication', 'cryptocurrency', 'education', 'electronics', 'energy', 'enterntainment', 'health', 'infrastructre', 'internet', 'investment', 'legal', 'manufacturing', 'media', 'platform.1', 'real estate', 'retail', 'smart contract', 'software', 'sports', 'tourism', 'virtual reality', 'other.1', 'name_std', 'symbol_std', 'softcap_usd', 'hardcap_usd', 'amount raised_usd', 'ico_successful', 'regulkyc dates_parsed', '__source__', '__key__']\n",
      "\n",
      "Yan: ['coin_ticker', 'received_money', 'sold_coins', 'role_of_token', 'category', 'goal', 'total_tokens', 'interest', 'fundraising_goal', 'start_end_date_coin_sell', 'ico_token_price', 'received_money.1', 'end_date', 'token_type', 'available_for_token_sale', 'min_max_personal_cap', 'whitelist', 'accepts', 'token_issue', 'cant_participate', 'end_date_clean', 'end_date_parsed', 'ico_successful', 'name_std', 'symbol_resolved', 'symbol_resolved_source', 'symbol_std', '__source__', '__key__']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Zenodo: {dfs[order[0]].columns.tolist()}\\n\")\n",
    "print(f\"ICPSR: {dfs[order[1]].columns.tolist()}\\n\")\n",
    "print(f\"Yan: {dfs[order[2]].columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0ddf5-e341-461e-a33a-83a5f3eeceae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c7533-de68-4d74-99a7-bbbceda02b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
