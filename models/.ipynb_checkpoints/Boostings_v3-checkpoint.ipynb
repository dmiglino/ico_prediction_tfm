{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c7e933-66f0-4ee3-8287-f93f93dbdab1",
   "metadata": {},
   "source": [
    "### Imports + carga de dataset + funciones utiles para imprimir metricas y guardar datos en disco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1bda787-f031-459e-b626-b2e86c708056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Utils: m√©tricas + leaderboard\n",
    "# ===========================\n",
    "import os, time, json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score, average_precision_score, brier_score_loss\n",
    ")\n",
    "\n",
    "def print_metrics(y_true, y_pred, y_proba=None, title=None):\n",
    "    \"\"\"Imprime m√©tricas al estilo RF que mostraste; si pas√°s y_proba agrega ROC-AUC, PR-AUC y Brier.\"\"\"\n",
    "    if title:\n",
    "        print(f\"\\n=== {title} ===\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\nüéØ Accuracy: {acc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    if y_proba is not None:\n",
    "        print(\"\\nProb-metrics:\")\n",
    "        print(f\"ROC-AUC : {roc_auc_score(y_true, y_proba):.4f}\")\n",
    "        print(f\"PR-AUC  : {average_precision_score(y_true, y_proba):.4f}\")\n",
    "        print(f\"Brier   : {brier_score_loss(y_true, y_proba):.4f}\")\n",
    "\n",
    "def save_experiment(exp_dir, model_tag, best_estimator, y_true, y_pred, y_proba, metrics_extra=None):\n",
    "    \"\"\"Guarda artefactos est√°ndar del experimento y devuelve dict con m√©tricas.\"\"\"\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    # M√©tricas base\n",
    "    mets = {\n",
    "        \"model\": model_tag,\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, y_proba)) if y_proba is not None else None,\n",
    "        \"pr_auc\": float(average_precision_score(y_true, y_proba)) if y_proba is not None else None,\n",
    "        \"f1\": float(f1_score(y_true, y_pred)),\n",
    "        \"brier\": float(brier_score_loss(y_true, y_proba)) if y_proba is not None else None,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"exp_dir\": exp_dir,\n",
    "    }\n",
    "    if metrics_extra:\n",
    "        mets.update(metrics_extra)\n",
    "\n",
    "    # Guardados\n",
    "    pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred, \"y_proba\": y_proba}).to_csv(\n",
    "        os.path.join(exp_dir, \"test_predictions.csv\"), index=False\n",
    "    )\n",
    "    with open(os.path.join(exp_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump({model_tag: mets}, f, indent=2)\n",
    "    joblib.dump(best_estimator, os.path.join(exp_dir, f\"{model_tag}.pkl\"))\n",
    "\n",
    "    # Leaderboard (append)\n",
    "    lb_path = \"experiments/leaderboard.csv\"\n",
    "    row = {k: mets[k] for k in [\"timestamp\",\"model\",\"accuracy\",\"roc_auc\",\"pr_auc\",\"brier\",\"exp_dir\"]}\n",
    "    df = pd.DataFrame([row])\n",
    "    os.makedirs(\"experiments\", exist_ok=True)\n",
    "    if os.path.exists(lb_path):\n",
    "        df.to_csv(lb_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(lb_path, index=False)\n",
    "    return mets\n",
    "\n",
    "DATA_PATH = \"../datasets/final/ico_dataset_final_v2_clean_enriquecido_feature_engineering_preico_v1.csv\"\n",
    "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100c770-684f-4f4c-8776-7e73b635fced",
   "metadata": {},
   "source": [
    "### XGBoost (grid + m√©tricas + save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a3dbdb-5d26-4bb2-9f84-9abe1e41c904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[XGB] Best params: Pipeline(memory=Memory(location=experiments/_sk_cache\\joblib),\n",
      "         steps=[('pre',\n",
      "                 ColumnTransformer(sparse_threshold=0.5,\n",
      "                                   transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='median'))]),\n",
      "                                                  ['total_tokens', 'ico_score',\n",
      "                                                   'start_year', 'start_qtr',\n",
      "                                                   'soft_cap', 'hard_cap',\n",
      "                                                   'token_price',\n",
      "                                                   'tokens_for_sale',\n",
      "                                                   'min_purchase',\n",
      "                                                   'max_purchase',\n",
      "                                                   'cap_ratio_sof...\n",
      "                               feature_types=None, feature_weights=None,\n",
      "                               gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=0.02,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=6, max_leaves=None, min_child_weight=1,\n",
      "                               missing=nan, monotone_constraints=None,\n",
      "                               multi_strategy=None, n_estimators=300, n_jobs=1,\n",
      "                               num_parallel_tree=None, ...))])\n",
      "\n",
      "=== XGBoost (raw) ===\n",
      "\n",
      "üéØ Accuracy: 0.8049\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8129    0.8505    0.8313       756\n",
      "           1     0.7934    0.7457    0.7688       582\n",
      "\n",
      "    accuracy                         0.8049      1338\n",
      "   macro avg     0.8032    0.7981    0.8001      1338\n",
      "weighted avg     0.8044    0.8049    0.8041      1338\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[643 113]\n",
      " [148 434]]\n",
      "\n",
      "Prob-metrics:\n",
      "ROC-AUC : 0.8816\n",
      "PR-AUC  : 0.8643\n",
      "Brier   : 0.1382\n",
      "\n",
      "üéØ Accuracy: 0.804932735426009\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8129    0.8505    0.8313       756\n",
      "           1     0.7934    0.7457    0.7688       582\n",
      "\n",
      "    accuracy                         0.8049      1338\n",
      "   macro avg     0.8032    0.7981    0.8001      1338\n",
      "weighted avg     0.8044    0.8049    0.8041      1338\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[643 113]\n",
      " [148 434]]\n",
      "\n",
      "Prob-metrics:\n",
      "ROC-AUC : 0.8815785287005219\n",
      "PR-AUC  : 0.8643269508129564\n",
      "Brier   : 0.13815736243155222\n",
      "\n",
      "=== XGBoost (calibrated) ===\n",
      "\n",
      "üéØ Accuracy: 0.8087\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7847    0.9114    0.8433       756\n",
      "           1     0.8543    0.6753    0.7543       582\n",
      "\n",
      "    accuracy                         0.8087      1338\n",
      "   macro avg     0.8195    0.7933    0.7988      1338\n",
      "weighted avg     0.8150    0.8087    0.8046      1338\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[689  67]\n",
      " [189 393]]\n",
      "\n",
      "Prob-metrics:\n",
      "ROC-AUC : 0.8794\n",
      "PR-AUC  : 0.8629\n",
      "Brier   : 0.1369\n",
      "CPU times: total: 4.02 s\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os, time, json, joblib\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.utils import check_random_state\n",
    "from joblib import Memory\n",
    "\n",
    "# -------- data\n",
    "DATA_PATH = \"../datasets/final/ico_dataset_final_v2_clean_enriquecido_feature_engineering_preico_v1.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "target = \"ico_successful\"\n",
    "df[target] = df[target].astype(int)\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "cat_cols = [c for c in [\"platform\",\"category\",\"location\"] if c in df.columns]\n",
    "bin_cols = [c for c in [\"mvp\",\"has_twitter\",\"has_facebook\",\"is_tax_regulated\",\"has_github\",\n",
    "                        \"has_reddit\",\"has_website\",\"has_whitepaper\",\"kyc\",\n",
    "                        \"accepts_BTC\",\"accepts_ETH\",\"has_contract_address\"] if c in df.columns]\n",
    "num_cols = [c for c in df.columns if c not in cat_cols + bin_cols + [target]]\n",
    "\n",
    "# -------- pre: sin scaler (√°rboles no lo precisan), OHE densa->sparse para acelerar\n",
    "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])  # <-- sin StandardScaler\n",
    "cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                     (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))])\n",
    "bin_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0))])\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    [(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols), (\"bin\", bin_pipe, bin_cols)],\n",
    "    remainder=\"drop\", sparse_threshold=0.5  # fuerza salida sparse si es posible\n",
    ")\n",
    "\n",
    "# -------- split\n",
    "X = df.drop(columns=[target]); y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)\n",
    "\n",
    "# -------- cache del pipeline para no re-OHE en cada fit\n",
    "CACHE_DIR = \"experiments/_sk_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "mem = Memory(location=CACHE_DIR, verbose=0)\n",
    "\n",
    "# -------- classifier: sin nested threads\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_jobs=1  # 1 para evitar anidado, se usa n_jobs en RandomizedSearch\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"clf\", xgb)], memory=mem)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rng = check_random_state(42)\n",
    "\n",
    "# -------- espacio de b√∫squeda m√°s acotado\n",
    "param_dist = {\n",
    "    # \"clf__n_estimators\": rng.randint(300, 1200, 10),\n",
    "    \"clf__n_estimators\": [300, 600, 900],\n",
    "    \"clf__learning_rate\": [0.02, 0.03, 0.05, 0.1],\n",
    "    \"clf__max_depth\": [3, 4, 5, 6],\n",
    "    \"clf__min_child_weight\": [1, 3, 5, 10],\n",
    "    \"clf__subsample\": [0.7, 0.85, 1.0],\n",
    "    \"clf__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"clf__reg_lambda\": [1.0, 2.0, 4.0],\n",
    "    \"clf__scale_pos_weight\": [1.0, 1.3, 1.6, 2.0],\n",
    "}\n",
    "\n",
    "# 48 iteraciones \n",
    "rs = RandomizedSearchCV(\n",
    "    pipe, param_distributions=param_dist, n_iter=48,\n",
    "    cv=cv, scoring=\"average_precision\",\n",
    "    n_jobs=4, \n",
    "    random_state=42, refit=True, verbose=1\n",
    ")\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "best_xgb = rs.best_estimator_\n",
    "y_proba = best_xgb.predict_proba(X_test)[:, 1]\n",
    "y_pred  = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "print(f\"[XGB] Best params: {best_xgb}\")\n",
    "print_metrics(y_test, y_pred, y_proba, title=\"XGBoost (raw)\")\n",
    "\n",
    "# -------- Prueba con Calibraci√≥n isot√≥nica\n",
    "xgb_cal = CalibratedClassifierCV(estimator=best_xgb, method=\"isotonic\", cv=5)\n",
    "xgb_cal.fit(X_train, y_train)\n",
    "y_proba_cal = xgb_cal.predict_proba(X_test)[:, 1]\n",
    "y_pred_cal  = (y_proba_cal >= 0.5).astype(int)\n",
    "print_metrics(y_test, y_pred_cal, y_proba_cal, title=\"XGBoost (calibrated)\")\n",
    "\n",
    "# -------- Grabado de experimento\n",
    "EXP_DIR = f\"experiments/xgboost_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "_ = save_experiment(EXP_DIR, \"xgboost_isotonic_calibrated\", xgb_cal, y_test.values, y_pred_cal, y_proba_cal,\n",
    "                    metrics_extra={\"best_params\": gs.best_params_})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee4b00-3c82-48f3-a975-27934e5974c4",
   "metadata": {},
   "source": [
    "### LightGBM (grid + m√©tricas + save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe6d958-dd90-4ae1-88af-b53a6f097cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# ===========================\n",
    "# LightGBM (grid + m√©tricas + save)\n",
    "# ===========================\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "target = \"ico_successful\"\n",
    "df[target] = df[target].astype(int)\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "cat_cols = [c for c in [\"platform\",\"category\",\"location\",\"caps_unit\"] if c in df.columns]\n",
    "bin_cols = [c for c in [\"mvp\",\"has_twitter\",\"has_facebook\",\"is_tax_regulated\",\"has_github\",\n",
    "                        \"has_reddit\",\"has_website\",\"has_whitepaper\",\"kyc\",\n",
    "                        \"accepts_BTC\",\"accepts_ETH\",\"has_contract_address\"] if c in df.columns]\n",
    "num_cols = [c for c in df.columns if c not in cat_cols + bin_cols + [target]]\n",
    "\n",
    "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))])\n",
    "bin_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0))])\n",
    "pre = ColumnTransformer([(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols), (\"bin\", bin_pipe, bin_cols)],\n",
    "                        remainder=\"drop\", sparse_threshold=0.3)\n",
    "\n",
    "X = df.drop(columns=[target]); y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)\n",
    "print(f\"Variables finales: {X.shape[1]}  |  Filas usadas: {X.shape[0]}\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "lgb = LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "lgb_pipe = Pipeline([(\"pre\", pre), (\"clf\", lgb)])\n",
    "lgb_grid = {\n",
    "    \"clf__n_estimators\": [400, 600, 800],\n",
    "    \"clf__num_leaves\": [31, 63, 127],\n",
    "    \"clf__max_depth\": [-1, 10, 20],\n",
    "    \"clf__learning_rate\": [0.03, 0.1],\n",
    "    \"clf__subsample\": [0.7, 1.0],\n",
    "    \"clf__colsample_bytree\": [0.6, 1.0]\n",
    "}\n",
    "gs = GridSearchCV(lgb_pipe, lgb_grid, cv=cv, scoring=\"average_precision\", n_jobs=-1, refit=True)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "best_lgb = gs.best_estimator_\n",
    "y_proba = best_lgb.predict_proba(X_test)[:, 1]\n",
    "y_pred  = (y_proba >= 0.5).astype(int)\n",
    "print(f\"[LGBM] Best params: {gs.best_params_}\")\n",
    "print_metrics(y_test, y_pred, y_proba, title=\"LightGBM (raw)\")\n",
    "\n",
    "# -------- Prueba con Calibraci√≥n isot√≥nica\n",
    "cal = CalibratedClassifierCV(estimator=best_lgb.named_steps[\"clf\"], method=\"isotonic\", cv=5)\n",
    "lgb_cal = Pipeline([(\"pre\", pre), (\"cal\", cal)])\n",
    "lgb_cal.fit(X_train, y_train)\n",
    "y_proba_cal = lgb_cal.predict_proba(X_test)[:, 1]\n",
    "y_pred_cal  = (y_proba_cal >= 0.5).astype(int)\n",
    "print_metrics(y_test, y_pred_cal, y_proba_cal, title=\"LightGBM (calibrated)\")\n",
    "\n",
    "EXP_DIR = f\"experiments/lightgbm_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "_ = save_experiment(EXP_DIR, \"lightgbm_isotonic_calibrated\", lgb_cal, y_test.values, y_pred_cal, y_proba_cal,\n",
    "                    metrics_extra={\"best_params\": gs.best_params_})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad4ae2-d5d4-4150-9a11-28c4b0428af4",
   "metadata": {},
   "source": [
    "### CatBoost (grid + m√©tricas + save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14554e72-5f4b-4a3c-9560-db190ccfabf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# ===========================\n",
    "# CatBoost (grid + m√©tricas + save)\n",
    "# ===========================\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "target = \"ico_successful\"\n",
    "df[target] = df[target].astype(int)\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "cat_cols = [c for c in [\"platform\",\"category\",\"location\",\"caps_unit\"] if c in df.columns]\n",
    "bin_cols = [c for c in [\"mvp\",\"has_twitter\",\"has_facebook\",\"is_tax_regulated\",\"has_github\",\n",
    "                        \"has_reddit\",\"has_website\",\"has_whitepaper\",\"kyc\",\n",
    "                        \"accepts_BTC\",\"accepts_ETH\",\"has_contract_address\"] if c in df.columns]\n",
    "num_cols = [c for c in df.columns if c not in cat_cols + bin_cols + [target]]\n",
    "\n",
    "num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))])\n",
    "bin_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0))])\n",
    "pre = ColumnTransformer([(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols), (\"bin\", bin_pipe, bin_cols)],\n",
    "                        remainder=\"drop\", sparse_threshold=0.3)\n",
    "\n",
    "X = df.drop(columns=[target]); y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)\n",
    "print(f\"Variables finales: {X.shape[1]}  |  Filas usadas: {X.shape[0]}\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cat = CatBoostClassifier(random_state=42, loss_function=\"Logloss\", verbose=False, allow_writing_files=False)\n",
    "cat_pipe = Pipeline([(\"pre\", pre), (\"clf\", cat)])\n",
    "cat_grid = {\n",
    "    \"clf__iterations\": [400, 600, 800],\n",
    "    \"clf__depth\": [4, 6, 8],\n",
    "    \"clf__learning_rate\": [0.03, 0.1],\n",
    "    \"clf__l2_leaf_reg\": [3.0, 5.0, 7.0],\n",
    "}\n",
    "gs = GridSearchCV(cat_pipe, cat_grid, cv=cv, scoring=\"average_precision\", n_jobs=-1, refit=True)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "best_cat = gs.best_estimator_\n",
    "y_proba = best_cat.predict_proba(X_test)[:, 1]\n",
    "y_pred  = (y_proba >= 0.5).astype(int)\n",
    "print(f\"[CAT] Best params: {gs.best_params_}\")\n",
    "print_metrics(y_test, y_pred, y_proba, title=\"CatBoost (raw)\")\n",
    "\n",
    "cal = CalibratedClassifierCV(estimator=best_cat.named_steps[\"clf\"], method=\"isotonic\", cv=5)\n",
    "cat_cal = Pipeline([(\"pre\", pre), (\"cal\", cal)])\n",
    "cat_cal.fit(X_train, y_train)\n",
    "y_proba_cal = cat_cal.predict_proba(X_test)[:, 1]\n",
    "y_pred_cal  = (y_proba_cal >= 0.5).astype(int)\n",
    "print_metrics(y_test, y_pred_cal, y_proba_cal, title=\"CatBoost (calibrated)\")\n",
    "\n",
    "EXP_DIR = f\"experiments/catboost_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "_ = save_experiment(EXP_DIR, \"catboost_isotonic_calibrated\", cat_cal, y_test.values, y_pred_cal, y_proba_cal,\n",
    "                    metrics_extra={\"best_params\": gs.best_params_})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f705e-9302-411f-b35b-6c08ebda16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Leaderboard (lee el CSV acumulado)\n",
    "# ===========================\n",
    "import pandas as pd\n",
    "\n",
    "lb_path = \"experiments/leaderboard.csv\"\n",
    "lb = pd.read_csv(lb_path)\n",
    "# Orden√° por PR-AUC y, de empate, por ROC-AUC (desc)\n",
    "lb_sorted = lb.sort_values(by=[\"pr_auc\",\"roc_auc\"], ascending=False).reset_index(drop=True)\n",
    "lb_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1188a-e9dd-4af1-97fa-eca3472e3759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
