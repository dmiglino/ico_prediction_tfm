{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffec2eaa-e219-443e-96cc-70cd0ff0ee5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado → 4457 filas, 30 columnas\n",
      "Filas train: 3119 | test: 1338 | vars: 29\n",
      "\n",
      "=== STACKING (meta logreg calibrada) ===\n",
      "ROC-AUC: 0.8842 | PR-AUC: 0.8681 | F1: 0.7635 | Acc: 0.8139 | Brier: 0.1342\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7924    0.9087    0.8466       756\n",
      "           1     0.8535    0.6907    0.7635       582\n",
      "\n",
      "    accuracy                         0.8139      1338\n",
      "   macro avg     0.8229    0.7997    0.8051      1338\n",
      "weighted avg     0.8190    0.8139    0.8105      1338\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[687  69]\n",
      " [180 402]]\n",
      "\n",
      "Prob-metrics:\n",
      "ROC-AUC : 0.8842\n",
      "PR-AUC  : 0.8681\n",
      "Brier   : 0.1342\n",
      "\n",
      "Artefactos en: experiments/stacking_v1\n",
      "CPU times: total: 5min 54s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ===========================================================\n",
    "# Stacking OOF: XGB + LGBM + CatBoost + RF (+ LogReg) -> Meta LogReg\n",
    "# Métrica objetivo: PR-AUC  | Guarda metrics y predicciones\n",
    "# ===========================================================\n",
    "import os, time, json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score,\n",
    "    average_precision_score, brier_score_loss, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\", category=UserWarning)\n",
    "\n",
    "DATA_PATH = \"../datasets/final/ico_dataset_final_v2_clean_enriquecido_feature_engineering_preico_v1.csv\"\n",
    "EXP_DIR = f\"experiments/stacking_v1\"\n",
    "os.makedirs(EXP_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- carga\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "target = \"ico_successful\"\n",
    "df[target] = df[target].astype(int)\n",
    "df = df.dropna(subset=[target])\n",
    "print(f\"Dataset cargado → {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "\n",
    "# ---------- columnas (adaptá si cambió algo)\n",
    "cat_cols = [c for c in [\"platform\",\"category\",\"location\"] if c in df.columns]\n",
    "bin_cols = [c for c in [\"mvp\",\"has_twitter\",\"has_facebook\",\"is_tax_regulated\",\"has_github\",\n",
    "                        \"has_reddit\",\"has_website\",\"has_whitepaper\",\"kyc\",\n",
    "                        \"accepts_BTC\",\"accepts_ETH\",\"has_contract_address\"] if c in df.columns]\n",
    "num_cols = [c for c in df.columns if c not in cat_cols + bin_cols + [target]]\n",
    "\n",
    "# ---------- preprocess para arboles (sin scaler) y para logreg (con scaler)\n",
    "pre_tree = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "        (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))]), cat_cols),\n",
    "        (\"bin\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=0))]), bin_cols),\n",
    "    ],\n",
    "    remainder=\"drop\", sparse_threshold=1.0\n",
    ")\n",
    "\n",
    "pre_log = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_cols),\n",
    "        (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))]), cat_cols),\n",
    "        (\"bin\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=0))]), bin_cols),\n",
    "    ],\n",
    "    remainder=\"drop\", sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "X = df.drop(columns=[target]); y = df[target]\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)\n",
    "print(f\"Filas train: {X_tr.shape[0]} | test: {X_te.shape[0]} | vars: {X_tr.shape[1]}\")\n",
    "\n",
    "# ---------- bases (parámetros razonables; ajustá según tus mejores)\n",
    "base_models = {\n",
    "    \"xgb\": Pipeline([(\"pre\", pre_tree),\n",
    "                     (\"clf\", XGBClassifier(\n",
    "                         objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "                         tree_method=\"hist\", random_state=42, n_jobs=1,\n",
    "                         n_estimators=1000, learning_rate=0.03, max_depth=4,\n",
    "                         min_child_weight=3, subsample=0.9, colsample_bytree=0.8, reg_lambda=2.0))]),\n",
    "    \"lgbm\": Pipeline([(\"pre\", pre_tree),\n",
    "                      (\"clf\", LGBMClassifier(\n",
    "                          objective=\"binary\", random_state=42, n_jobs=1, verbose=-1,\n",
    "                          n_estimators=1500, learning_rate=0.03, num_leaves=63,\n",
    "                          min_child_samples=80, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1))]),\n",
    "    \"cat\": Pipeline([(\"pre\", pre_tree),\n",
    "                     (\"clf\", CatBoostClassifier(\n",
    "                         loss_function=\"Logloss\", eval_metric=\"PRAUC\", random_state=42,\n",
    "                         iterations=1500, learning_rate=0.03, depth=6, l2_leaf_reg=5.0,\n",
    "                         verbose=False, allow_writing_files=False))]),\n",
    "    \"rf\":  Pipeline([(\"pre\", pre_tree),\n",
    "                     (\"clf\", __import__(\"sklearn\").ensemble.RandomForestClassifier(\n",
    "                         n_estimators=1000, max_depth=None, min_samples_leaf=2, n_jobs=1, random_state=42))]),\n",
    "    \"log\": Pipeline([(\"pre\", pre_log),\n",
    "                     (\"clf\", LogisticRegression(\n",
    "                         C=1.0, solver=\"lbfgs\", max_iter=2000, n_jobs=1))])\n",
    "}\n",
    "\n",
    "# ---------- OOF para meta\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_meta = pd.DataFrame(index=X_tr.index)\n",
    "test_meta = pd.DataFrame(index=X_te.index)\n",
    "\n",
    "for name, pipe in base_models.items():\n",
    "    oof_pred = np.zeros(X_tr.shape[0], dtype=float)\n",
    "    test_pred = np.zeros(X_te.shape[0], dtype=float)\n",
    "    for tr_idx, va_idx in skf.split(X_tr, y_tr):\n",
    "        X_tr_i, X_va_i = X_tr.iloc[tr_idx], X_tr.iloc[va_idx]\n",
    "        y_tr_i, y_va_i = y_tr.iloc[tr_idx], y_tr.iloc[va_idx]\n",
    "        pipe.fit(X_tr_i, y_tr_i)\n",
    "        oof_pred[va_idx] = pipe.predict_proba(X_va_i)[:, 1]\n",
    "    # fit en todo el train y predecí test\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    test_pred[:] = pipe.predict_proba(X_te)[:, 1]\n",
    "    oof_meta[name] = oof_pred\n",
    "    test_meta[name] = test_pred\n",
    "    # opcional: guardar cada base ya entrenada en full train\n",
    "    joblib.dump(pipe, os.path.join(EXP_DIR, f\"base_{name}.pkl\"))\n",
    "\n",
    "# ---------- meta-modelo (logistic) + calibración isotónica\n",
    "meta = LogisticRegression(C=1.0, solver=\"lbfgs\", max_iter=1000)\n",
    "cal_meta = CalibratedClassifierCV(estimator=meta, method=\"isotonic\", cv=5)\n",
    "cal_meta.fit(oof_meta, y_tr)\n",
    "\n",
    "y_proba = cal_meta.predict_proba(test_meta)[:, 1]\n",
    "y_pred  = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "# ---------- métricas y guardado\n",
    "m = {\n",
    "    \"model\": \"stacking_oof_hybrib\",\n",
    "    \"roc_auc\": float(roc_auc_score(y_te, y_proba)),\n",
    "    \"pr_auc\": float(average_precision_score(y_te, y_proba)),\n",
    "    \"f1\": float(f1_score(y_te, y_pred)),\n",
    "    \"accuracy\": float(accuracy_score(y_te, y_pred)),\n",
    "    \"brier\": float(brier_score_loss(y_te, y_proba)),\n",
    "    \"confusion_matrix\": confusion_matrix(y_te, y_pred).tolist(),\n",
    "}\n",
    "\n",
    "print(\"\\n=== STACKING (meta logreg calibrada) ===\")\n",
    "print(f\"ROC-AUC: {m['roc_auc']:.4f} | PR-AUC: {m['pr_auc']:.4f} | F1: {m['f1']:.4f} | Acc: {m['accuracy']:.4f} | Brier: {m['brier']:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_te, y_pred, digits=4))\n",
    "print(\"\\nConfusion Matrix:\\n\", np.array(m[\"confusion_matrix\"]))\n",
    "\n",
    "if y_proba is not None:\n",
    "    print(\"\\nProb-metrics:\")\n",
    "    print(f\"ROC-AUC : {roc_auc_score(y_te, y_proba):.4f}\")\n",
    "    print(f\"PR-AUC  : {average_precision_score(y_te, y_proba):.4f}\")\n",
    "    print(f\"Brier   : {brier_score_loss(y_te, y_proba):.4f}\")\n",
    "        \n",
    "# guardar artefactos\n",
    "with open(os.path.join(EXP_DIR, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump({\"stacking_oof_hybrib\": m}, f, indent=2)\n",
    "pd.DataFrame({\"y_true\": y_te.values, \"y_proba\": y_proba, \"y_pred\": y_pred}).to_csv(\n",
    "    os.path.join(EXP_DIR, \"test_predictions.csv\"), index=False\n",
    ")\n",
    "joblib.dump(cal_meta, os.path.join(EXP_DIR, \"stacking_meta.pkl\"))\n",
    "print(\"\\nArtefactos en:\", EXP_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31a3a0a9-5beb-43c3-bf28-993eaa92f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3119, 29), Test: (1338, 29)\n",
      "Meta features: ['xgb', 'lgbm', 'cat', 'rf', 'log', 'xgb_logit', 'xgb_rank', 'lgbm_logit', 'lgbm_rank', 'cat_logit', 'cat_rank', 'rf_logit', 'rf_rank', 'log_logit', 'log_rank']\n",
      "\n",
      "[Meta-XGB]     {'pr_auc': 0.8664949034833431, 'roc_auc': 0.8825205912834779, 'f1': 0.7674858223062382, 'accuracy': 0.8161434977578476, 'brier': 0.13543348333999988, 'confusion_matrix': [[686, 70], [176, 406]]}\n",
      "[Blend]        {'pr_auc': 0.8625960655287102, 'roc_auc': 0.8800910016545755, 'f1': 0.764378478664193, 'accuracy': 0.8101644245142003, 'brier': 0.13887237819930862, 'confusion_matrix': [[672, 84], [170, 412]]}\n",
      "[Hybrid a=0.5]  {'pr_auc': 0.8670382623100331, 'roc_auc': 0.8839137984326988, 'f1': 0.7679245283018868, 'accuracy': 0.8161434977578476, 'brier': 0.13565365214192857, 'confusion_matrix': [[685, 71], [175, 407]]}\n",
      "\n",
      "Pesos blend:\n",
      "       xgb: 0.200\n",
      "      lgbm: 0.200\n",
      "       cat: 0.200\n",
      "        rf: 0.200\n",
      "       log: 0.200\n",
      "\n",
      ">>> Mejor meta: hybrid_0.5 | PR-AUC=0.8670 ROC-AUC=0.8839 Brier=0.1357\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7965    0.9061    0.8478       756\n",
      "           1     0.8515    0.6993    0.7679       582\n",
      "\n",
      "    accuracy                         0.8161      1338\n",
      "   macro avg     0.8240    0.8027    0.8078      1338\n",
      "weighted avg     0.8204    0.8161    0.8130      1338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Stacking OOF con meta-features + meta-XGB + blending\n",
    "# (versión coherente, sin errores de índice)\n",
    "# ===========================================\n",
    "import warnings, os, time, json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import (average_precision_score, roc_auc_score, f1_score,\n",
    "                             accuracy_score, brier_score_loss, classification_report, confusion_matrix)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.special import logit\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- silenciar warning de feature names de LGBM\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\", category=UserWarning)\n",
    "\n",
    "# ---------- carga\n",
    "DATA_PATH = \"../datasets/final/ico_dataset_final_v2_clean_enriquecido_feature_engineering_preico_v1.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "target = \"ico_successful\"\n",
    "df[target] = df[target].astype(int)\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "# ---------- columnas (ajustá si cambió algo)\n",
    "cat_cols = [c for c in [\"platform\",\"category\",\"location\",\"caps_unit\"] if c in df.columns]\n",
    "bin_cols = [c for c in [\"mvp\",\"has_twitter\",\"has_facebook\",\"is_tax_regulated\",\"has_github\",\n",
    "                        \"has_reddit\",\"has_website\",\"has_whitepaper\",\"kyc\",\n",
    "                        \"accepts_BTC\",\"accepts_ETH\",\"has_contract_address\"] if c in df.columns]\n",
    "num_cols = [c for c in df.columns if c not in cat_cols + bin_cols + [target]]\n",
    "\n",
    "# ---------- preprocess para árboles (sin scaler) y para logreg (con scaler)\n",
    "pre_tree = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "        (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))]), cat_cols),\n",
    "        (\"bin\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=0))]), bin_cols),\n",
    "    ],\n",
    "    remainder=\"drop\", sparse_threshold=1.0\n",
    ")\n",
    "pre_log = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_cols),\n",
    "        (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))]), cat_cols),\n",
    "        (\"bin\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=0))]), bin_cols),\n",
    "    ],\n",
    "    remainder=\"drop\", sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# ---------- split base\n",
    "X = df.drop(columns=[target]); y = df[target]\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)\n",
    "print(f\"Train: {X_tr.shape}, Test: {X_te.shape}\")\n",
    "\n",
    "# ---------- modelos base (parámetros razonables)\n",
    "base_models = {\n",
    "    \"xgb\": Pipeline([(\"pre\", pre_tree),\n",
    "                     (\"clf\", XGBClassifier(\n",
    "                         objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "                         tree_method=\"hist\", random_state=42, n_jobs=1,\n",
    "                         n_estimators=800, learning_rate=0.03, max_depth=4,\n",
    "                         min_child_weight=3, subsample=0.9, colsample_bytree=0.8, reg_lambda=2.0))]),\n",
    "    \"lgbm\": Pipeline([(\"pre\", pre_tree),\n",
    "                      (\"clf\", LGBMClassifier(\n",
    "                          objective=\"binary\", random_state=42, n_jobs=1, verbose=-1,\n",
    "                          n_estimators=1200, learning_rate=0.03, num_leaves=63,\n",
    "                          min_child_samples=80, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1))]),\n",
    "    \"cat\": Pipeline([(\"pre\", pre_tree),\n",
    "                     (\"clf\", CatBoostClassifier(\n",
    "                         loss_function=\"Logloss\",\n",
    "                         eval_metric=\"PRAUC\",      # <— compatible con versiones más viejas\n",
    "                         iterations=1200, learning_rate=0.03, depth=6, l2_leaf_reg=5.0,\n",
    "                         random_state=42, verbose=False, allow_writing_files=False))]),\n",
    "    \"rf\":  Pipeline([(\"pre\", pre_tree),\n",
    "                     (\"clf\", RandomForestClassifier(\n",
    "                         n_estimators=600, max_depth=None, min_samples_leaf=2, n_jobs=1, random_state=42))]),\n",
    "    \"log\": Pipeline([(\"pre\", pre_log),\n",
    "                     (\"clf\", LogisticRegression(\n",
    "                         C=1.0, solver=\"lbfgs\", max_iter=2000, n_jobs=1))]),\n",
    "}\n",
    "\n",
    "# ---------- OOF sin perder alineación\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_meta = pd.DataFrame(index=X_tr.index)\n",
    "test_meta = pd.DataFrame(index=X_te.index)\n",
    "\n",
    "for name, pipe in base_models.items():\n",
    "    oof_pred = np.zeros(X_tr.shape[0], dtype=float)\n",
    "    test_pred = np.zeros(X_te.shape[0], dtype=float)\n",
    "    for tr_idx, va_idx in skf.split(X_tr, y_tr):\n",
    "        # usar posiciones -> luego mapear a índices\n",
    "        X_tr_i, X_va_i = X_tr.iloc[tr_idx], X_tr.iloc[va_idx]\n",
    "        y_tr_i, y_va_i = y_tr.iloc[tr_idx], y_tr.iloc[va_idx]\n",
    "        pipe.fit(X_tr_i, y_tr_i)\n",
    "        oof_pred[va_idx] = pipe.predict_proba(X_va_i)[:, 1]\n",
    "    # fit final en todo el train y predecir test\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    test_pred[:] = pipe.predict_proba(X_te)[:, 1]\n",
    "    oof_meta[name] = oof_pred\n",
    "    test_meta[name] = test_pred\n",
    "\n",
    "# ---------- meta-features (proba + logit + rank)\n",
    "def safe_logit(p, eps=1e-6):\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return logit(p)\n",
    "\n",
    "def rank01(a):\n",
    "    order = pd.Series(a).rank(method=\"average\")\n",
    "    return (order - order.min()) / (order.max() - order.min() + 1e-12)\n",
    "\n",
    "use_cols = [c for c in [\"xgb\",\"lgbm\",\"cat\",\"rf\",\"log\"] if c in oof_meta.columns]\n",
    "Z_tr = oof_meta[use_cols].copy()\n",
    "Z_te = test_meta[use_cols].copy()\n",
    "\n",
    "for c in use_cols:\n",
    "    Z_tr[f\"{c}_logit\"] = safe_logit(Z_tr[c].values)\n",
    "    Z_tr[f\"{c}_rank\"]  = rank01(Z_tr[c].values)\n",
    "    Z_te[f\"{c}_logit\"] = safe_logit(Z_te[c].values)\n",
    "    Z_te[f\"{c}_rank\"]  = rank01(Z_te[c].values)\n",
    "\n",
    "logit_rank_cols = [c for c in Z_tr.columns if c.endswith(\"_logit\") or c.endswith(\"_rank\")]\n",
    "sc = StandardScaler()\n",
    "Z_tr[logit_rank_cols] = sc.fit_transform(Z_tr[logit_rank_cols])\n",
    "Z_te[logit_rank_cols] = sc.transform(Z_te[logit_rank_cols])\n",
    "\n",
    "print(\"Meta features:\", list(Z_tr.columns))\n",
    "\n",
    "# ---------- meta-XGB con early stopping (uso split por POSICIONES, no por índices)\n",
    "idx_all = np.arange(len(Z_tr))\n",
    "idx_tr_in, idx_val_in = train_test_split(idx_all, test_size=0.2, stratify=y_tr.values, random_state=42)\n",
    "\n",
    "Z_tr_in, Z_val_in = Z_tr.iloc[idx_tr_in], Z_tr.iloc[idx_val_in]\n",
    "y_tr_in, y_val_in = y_tr.iloc[idx_tr_in], y_tr.iloc[idx_val_in]\n",
    "\n",
    "meta_xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": [300, 500, 800, 1000],\n",
    "    \"learning_rate\": [0.02, 0.03, 0.05, 0.1],\n",
    "    \"max_depth\": [2, 3],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"subsample\": [0.7, 0.85, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"reg_lambda\": [0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    meta_xgb, param_distributions=param_dist, n_iter=25,\n",
    "    scoring=\"average_precision\", cv=3, random_state=42, n_jobs=4, refit=False, verbose=0\n",
    ")\n",
    "rs.fit(Z_tr_in, y_tr_in)\n",
    "best_params = rs.best_params_\n",
    "\n",
    "meta_best = XGBClassifier(objective=\"binary:logistic\", eval_metric=\"aucpr\", tree_method=\"hist\",\n",
    "                          n_jobs=1, random_state=42, **best_params)\n",
    "meta_best.fit(\n",
    "    Z_tr_in, y_tr_in,\n",
    "    eval_set=[(Z_val_in, y_val_in)],\n",
    "    verbose=False,\n",
    "    #early_stopping_rounds=100\n",
    ")\n",
    "proba_meta = meta_best.predict_proba(Z_te)[:, 1]\n",
    "\n",
    "# ---------- blending (pesos óptimos) — usar POSICIONES\n",
    "P_tr = oof_meta[use_cols].values   # (n_tr, K)\n",
    "P_te = test_meta[use_cols].values  # (n_te, K)\n",
    "y_tr_arr = y_tr.values\n",
    "y_te_arr = y_te.values\n",
    "\n",
    "P_tr_in = P_tr[idx_tr_in, :]\n",
    "P_val_in = P_tr[idx_val_in, :]\n",
    "y_tr_in_arr = y_tr_arr[idx_tr_in]\n",
    "y_val_in_arr = y_tr_arr[idx_val_in]\n",
    "\n",
    "def ap_neg(w):\n",
    "    w = np.clip(w, 0, None)\n",
    "    s = w.sum()\n",
    "    if s <= 1e-12:\n",
    "        return 1.0\n",
    "    w = w / s\n",
    "    p = P_val_in @ w\n",
    "    return -average_precision_score(y_val_in_arr, p)\n",
    "\n",
    "K = P_tr.shape[1]\n",
    "w0 = np.ones(K) / K\n",
    "bounds = [(0.0, 1.0)] * K\n",
    "cons = ({\"type\":\"eq\", \"fun\": lambda w: np.sum(np.clip(w,0,None)) - 1.0},)\n",
    "\n",
    "opt = minimize(ap_neg, w0, method=\"SLSQP\", bounds=bounds, constraints=cons, options={\"maxiter\":200, \"ftol\":1e-8})\n",
    "w_star = np.clip(opt.x, 0, None); w_star /= (w_star.sum() + 1e-12)\n",
    "\n",
    "proba_blend = P_te @ w_star\n",
    "\n",
    "# ---------- híbrido (promedio ponderado meta vs blend)\n",
    "alpha = 0.5\n",
    "proba_hybrid = alpha * proba_meta + (1 - alpha) * proba_blend\n",
    "\n",
    "def metrics_from_proba(y_true, y_proba, thr=0.5):\n",
    "    y_pred = (y_proba >= thr).astype(int)\n",
    "    return dict(\n",
    "        pr_auc=float(average_precision_score(y_true, y_proba)),\n",
    "        roc_auc=float(roc_auc_score(y_true, y_proba)),\n",
    "        f1=float(f1_score(y_true, y_pred)),\n",
    "        accuracy=float(accuracy_score(y_true, y_pred)),\n",
    "        brier=float(brier_score_loss(y_true, y_proba)),\n",
    "        confusion_matrix=confusion_matrix(y_true, y_pred).tolist(),\n",
    "    )\n",
    "\n",
    "m_meta   = metrics_from_proba(y_te_arr, proba_meta)\n",
    "m_blend  = metrics_from_proba(y_te_arr, proba_blend)\n",
    "m_hybrid = metrics_from_proba(y_te_arr, proba_hybrid)\n",
    "\n",
    "print(\"\\n[Meta-XGB]    \", m_meta)\n",
    "print(\"[Blend]       \", m_blend)\n",
    "print(f\"[Hybrid a={alpha}] \", m_hybrid)\n",
    "print(\"\\nPesos blend:\")\n",
    "for name, w in zip(use_cols, w_star):\n",
    "    print(f\"  {name:>8s}: {w:.3f}\")\n",
    "\n",
    "# ---------- elegir mejor por PR-AUC\n",
    "best_name, best_proba, best_m = max(\n",
    "    [(\"meta_xgb\", proba_meta, m_meta), (\"blend\", proba_blend, m_blend), (f\"hybrid_{alpha}\", proba_hybrid, m_hybrid)],\n",
    "    key=lambda t: t[2][\"pr_auc\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> Mejor meta: {best_name} | PR-AUC={best_m['pr_auc']:.4f} ROC-AUC={best_m['roc_auc']:.4f} Brier={best_m['brier']:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_te_arr, (best_proba>=0.5).astype(int), digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a31c129-d406-4598-8396-bc31eeecfa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3119, 29), Test: (1338, 29)\n",
      "Meta features: ['xgb', 'lgbm', 'cat', 'rf', 'log', 'xgb_logit', 'xgb_rank', 'lgbm_logit', 'lgbm_rank', 'cat_logit', 'cat_rank', 'rf_logit', 'rf_rank', 'log_logit', 'log_rank']\n",
      "\n",
      "[Meta-XGB]     {'pr_auc': 0.867691574089774, 'roc_auc': 0.8830035546100838, 'f1': 0.7700934579439253, 'accuracy': 0.8161434977578476, 'brier': 0.1348992476458625, 'confusion_matrix': [[680, 76], [170, 412]]}\n",
      "[Blend]        {'pr_auc': 0.8630318013533859, 'roc_auc': 0.879834178803251, 'f1': 0.7663551401869159, 'accuracy': 0.8131539611360239, 'brier': 0.13918552965260406, 'confusion_matrix': [[678, 78], [172, 410]]}\n",
      "[Hybrid a=0.5]  {'pr_auc': 0.8682478726777915, 'roc_auc': 0.8844774450444554, 'f1': 0.7642124883504194, 'accuracy': 0.8109118086696562, 'brier': 0.13539744398406436, 'confusion_matrix': [[675, 81], [172, 410]]}\n",
      "\n",
      "Pesos blend:\n",
      "       xgb: 0.200\n",
      "      lgbm: 0.200\n",
      "       cat: 0.200\n",
      "        rf: 0.200\n",
      "       log: 0.200\n",
      "\n",
      ">>> Mejor meta: hybrid_0.5 | PR-AUC=0.8682 ROC-AUC=0.8845 Brier=0.1354\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7969    0.8929    0.8422       756\n",
      "           1     0.8350    0.7045    0.7642       582\n",
      "\n",
      "    accuracy                         0.8109      1338\n",
      "   macro avg     0.8160    0.7987    0.8032      1338\n",
      "weighted avg     0.8135    0.8109    0.8083      1338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Stacking OOF con meta-features + meta-XGB + blending\n",
    "# (versión coherente, sin errores de índice)\n",
    "# ===========================================\n",
    "import warnings, os, time, json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import (average_precision_score, roc_auc_score, f1_score,\n",
    "                             accuracy_score, brier_score_loss, classification_report, confusion_matrix)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.special import logit\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- silenciar warning de feature names de LGBM\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\", category=UserWarning)\n",
    "\n",
    "# ---------- carga\n",
    "DATA_PATH = \"../datasets/final/ico_dataset_final_v2_clean_enriquecido_feature_engineering_preico_v1.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "target = \"ico_successful\"\n",
    "df[target] = df[target].astype(int)\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "# ---------- columnas (ajustá si cambió algo)\n",
    "cat_cols = [c for c in [\"platform\",\"category\",\"location\",\"caps_unit\"] if c in df.columns]\n",
    "bin_cols = [c for c in [\"mvp\",\"has_twitter\",\"has_facebook\",\"is_tax_regulated\",\"has_github\",\n",
    "                        \"has_reddit\",\"has_website\",\"has_whitepaper\",\"kyc\",\n",
    "                        \"accepts_BTC\",\"accepts_ETH\",\"has_contract_address\"] if c in df.columns]\n",
    "num_cols = [c for c in df.columns if c not in cat_cols + bin_cols + [target]]\n",
    "\n",
    "# ---------- preprocess para árboles (sin scaler) y para logreg (con scaler)\n",
    "pre_tree = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "        (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))]), cat_cols),\n",
    "        (\"bin\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=0))]), bin_cols),\n",
    "    ],\n",
    "    remainder=\"drop\", sparse_threshold=1.0\n",
    ")\n",
    "pre_log = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_cols),\n",
    "        (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))]), cat_cols),\n",
    "        (\"bin\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=0))]), bin_cols),\n",
    "    ],\n",
    "    remainder=\"drop\", sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# ---------- split base\n",
    "X = df.drop(columns=[target]); y = df[target]\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)\n",
    "print(f\"Train: {X_tr.shape}, Test: {X_te.shape}\")\n",
    "\n",
    "# ---------- modelos base (parámetros razonables)\n",
    "base_models = {\n",
    "    \"xgb\": Pipeline([(\"pre\", pre_tree),\n",
    "                     (\"clf\", XGBClassifier(\n",
    "                         objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "                         tree_method=\"hist\", random_state=42, n_jobs=1,\n",
    "                         n_estimators=1200, learning_rate=0.03, max_depth=4,\n",
    "                         min_child_weight=3, subsample=0.9, colsample_bytree=0.8, reg_lambda=2.0))]),\n",
    "    \"lgbm\": Pipeline([(\"pre\", pre_tree),\n",
    "                      (\"clf\", LGBMClassifier(\n",
    "                          objective=\"binary\", random_state=42, n_jobs=1, verbose=-1,\n",
    "                          n_estimators=1200, learning_rate=0.03, num_leaves=63,\n",
    "                          min_child_samples=80, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1))]),\n",
    "    \"cat\": Pipeline([(\"pre\", pre_tree),\n",
    "                     (\"clf\", CatBoostClassifier(\n",
    "                         loss_function=\"Logloss\",\n",
    "                         eval_metric=\"PRAUC\",      # <— compatible con versiones más viejas\n",
    "                         iterations=1200, learning_rate=0.03, depth=6, l2_leaf_reg=5.0,\n",
    "                         random_state=42, verbose=False, allow_writing_files=False))]),\n",
    "    \"rf\":  Pipeline([(\"pre\", pre_tree),\n",
    "                     (\"clf\", RandomForestClassifier(\n",
    "                         n_estimators=1200, max_depth=None, min_samples_leaf=2, n_jobs=1, random_state=42))]),\n",
    "    \"log\": Pipeline([(\"pre\", pre_log),\n",
    "                     (\"clf\", LogisticRegression(\n",
    "                         C=1.0, solver=\"lbfgs\", max_iter=2000, n_jobs=1))]),\n",
    "}\n",
    "\n",
    "# ---------- OOF sin perder alineación\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "oof_meta = pd.DataFrame(index=X_tr.index)\n",
    "test_meta = pd.DataFrame(index=X_te.index)\n",
    "\n",
    "for name, pipe in base_models.items():\n",
    "    oof_sum = np.zeros(X_tr.shape[0], dtype=float)\n",
    "    oof_cnt = np.zeros(X_tr.shape[0], dtype=float)\n",
    "    test_sum = np.zeros(X_te.shape[0], dtype=float)\n",
    "\n",
    "    for tr_idx, va_idx in rskf.split(X_tr, y_tr):\n",
    "        Xtr, Xva = X_tr.iloc[tr_idx], X_tr.iloc[va_idx]\n",
    "        ytr, yva = y_tr.iloc[tr_idx], y_tr.iloc[va_idx]\n",
    "        pipe.fit(Xtr, ytr)\n",
    "        p = pipe.predict_proba(Xva)[:,1]\n",
    "        oof_sum[va_idx] += p\n",
    "        oof_cnt[va_idx] += 1.0\n",
    "        test_sum += pipe.predict_proba(X_te)[:,1] / rskf.get_n_splits()\n",
    "\n",
    "    oof_meta[name] = np.divide(oof_sum, oof_cnt, out=np.zeros_like(oof_sum), where=oof_cnt>0)\n",
    "    test_meta[name] = test_sum\n",
    "\n",
    "# ---------- meta-features (proba + logit + rank)\n",
    "def safe_logit(p, eps=1e-6):\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return logit(p)\n",
    "\n",
    "def rank01(a):\n",
    "    order = pd.Series(a).rank(method=\"average\")\n",
    "    return (order - order.min()) / (order.max() - order.min() + 1e-12)\n",
    "\n",
    "use_cols = [c for c in [\"xgb\",\"lgbm\",\"cat\",\"rf\",\"log\"] if c in oof_meta.columns]\n",
    "Z_tr = oof_meta[use_cols].copy()\n",
    "Z_te = test_meta[use_cols].copy()\n",
    "\n",
    "for c in use_cols:\n",
    "    Z_tr[f\"{c}_logit\"] = safe_logit(Z_tr[c].values)\n",
    "    Z_tr[f\"{c}_rank\"]  = rank01(Z_tr[c].values)\n",
    "    Z_te[f\"{c}_logit\"] = safe_logit(Z_te[c].values)\n",
    "    Z_te[f\"{c}_rank\"]  = rank01(Z_te[c].values)\n",
    "\n",
    "logit_rank_cols = [c for c in Z_tr.columns if c.endswith(\"_logit\") or c.endswith(\"_rank\")]\n",
    "sc = StandardScaler()\n",
    "Z_tr[logit_rank_cols] = sc.fit_transform(Z_tr[logit_rank_cols])\n",
    "Z_te[logit_rank_cols] = sc.transform(Z_te[logit_rank_cols])\n",
    "\n",
    "print(\"Meta features:\", list(Z_tr.columns))\n",
    "\n",
    "# ---------- meta-XGB con early stopping (uso split por POSICIONES, no por índices)\n",
    "idx_all = np.arange(len(Z_tr))\n",
    "idx_tr_in, idx_val_in = train_test_split(idx_all, test_size=0.2, stratify=y_tr.values, random_state=42)\n",
    "\n",
    "Z_tr_in, Z_val_in = Z_tr.iloc[idx_tr_in], Z_tr.iloc[idx_val_in]\n",
    "y_tr_in, y_val_in = y_tr.iloc[idx_tr_in], y_tr.iloc[idx_val_in]\n",
    "\n",
    "meta_xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": [300, 500, 800, 1000],\n",
    "    \"learning_rate\": [0.02, 0.03, 0.05, 0.1],\n",
    "    \"max_depth\": [2, 3],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"subsample\": [0.7, 0.85, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"reg_lambda\": [0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    meta_xgb, param_distributions=param_dist, n_iter=25,\n",
    "    scoring=\"average_precision\", cv=3, random_state=42, n_jobs=4, refit=False, verbose=0\n",
    ")\n",
    "rs.fit(Z_tr_in, y_tr_in)\n",
    "best_params = rs.best_params_\n",
    "\n",
    "meta_best = XGBClassifier(objective=\"binary:logistic\", eval_metric=\"aucpr\", tree_method=\"hist\",\n",
    "                          n_jobs=1, random_state=42, **best_params)\n",
    "meta_best.fit(\n",
    "    Z_tr_in, y_tr_in,\n",
    "    eval_set=[(Z_val_in, y_val_in)],\n",
    "    verbose=False,\n",
    "    #early_stopping_rounds=100\n",
    ")\n",
    "proba_meta = meta_best.predict_proba(Z_te)[:, 1]\n",
    "\n",
    "# ---------- blending (pesos óptimos) — usar POSICIONES\n",
    "P_tr = oof_meta[use_cols].values   # (n_tr, K)\n",
    "P_te = test_meta[use_cols].values  # (n_te, K)\n",
    "y_tr_arr = y_tr.values\n",
    "y_te_arr = y_te.values\n",
    "\n",
    "P_tr_in = P_tr[idx_tr_in, :]\n",
    "P_val_in = P_tr[idx_val_in, :]\n",
    "y_tr_in_arr = y_tr_arr[idx_tr_in]\n",
    "y_val_in_arr = y_tr_arr[idx_val_in]\n",
    "\n",
    "def ap_neg(w):\n",
    "    w = np.clip(w, 0, None)\n",
    "    s = w.sum()\n",
    "    if s <= 1e-12:\n",
    "        return 1.0\n",
    "    w = w / s\n",
    "    p = P_val_in @ w\n",
    "    return -average_precision_score(y_val_in_arr, p)\n",
    "\n",
    "K = P_tr.shape[1]\n",
    "w0 = np.ones(K) / K\n",
    "bounds = [(0.0, 1.0)] * K\n",
    "cons = ({\"type\":\"eq\", \"fun\": lambda w: np.sum(np.clip(w,0,None)) - 1.0},)\n",
    "\n",
    "opt = minimize(ap_neg, w0, method=\"SLSQP\", bounds=bounds, constraints=cons, options={\"maxiter\":200, \"ftol\":1e-8})\n",
    "w_star = np.clip(opt.x, 0, None); w_star /= (w_star.sum() + 1e-12)\n",
    "\n",
    "proba_blend = P_te @ w_star\n",
    "\n",
    "# ---------- híbrido (promedio ponderado meta vs blend)\n",
    "alpha = 0.5\n",
    "proba_hybrid = alpha * proba_meta + (1 - alpha) * proba_blend\n",
    "\n",
    "def metrics_from_proba(y_true, y_proba, thr=0.5):\n",
    "    y_pred = (y_proba >= thr).astype(int)\n",
    "    return dict(\n",
    "        pr_auc=float(average_precision_score(y_true, y_proba)),\n",
    "        roc_auc=float(roc_auc_score(y_true, y_proba)),\n",
    "        f1=float(f1_score(y_true, y_pred)),\n",
    "        accuracy=float(accuracy_score(y_true, y_pred)),\n",
    "        brier=float(brier_score_loss(y_true, y_proba)),\n",
    "        confusion_matrix=confusion_matrix(y_true, y_pred).tolist(),\n",
    "    )\n",
    "\n",
    "m_meta   = metrics_from_proba(y_te_arr, proba_meta)\n",
    "m_blend  = metrics_from_proba(y_te_arr, proba_blend)\n",
    "m_hybrid = metrics_from_proba(y_te_arr, proba_hybrid)\n",
    "\n",
    "print(\"\\n[Meta-XGB]    \", m_meta)\n",
    "print(\"[Blend]       \", m_blend)\n",
    "print(f\"[Hybrid a={alpha}] \", m_hybrid)\n",
    "print(\"\\nPesos blend:\")\n",
    "for name, w in zip(use_cols, w_star):\n",
    "    print(f\"  {name:>8s}: {w:.3f}\")\n",
    "\n",
    "# ---------- elegir mejor por PR-AUC\n",
    "best_name, best_proba, best_m = max(\n",
    "    [(\"meta_xgb\", proba_meta, m_meta), (\"blend\", proba_blend, m_blend), (f\"hybrid_{alpha}\", proba_hybrid, m_hybrid)],\n",
    "    key=lambda t: t[2][\"pr_auc\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> Mejor meta: {best_name} | PR-AUC={best_m['pr_auc']:.4f} ROC-AUC={best_m['roc_auc']:.4f} Brier={best_m['brier']:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_te_arr, (best_proba>=0.5).astype(int), digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b00768-a462-4ca8-ab61-555739ee512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC-AUC: 0.8842 | PR-AUC: 0.8681 | F1: 0.7635 | Acc: 0.8139 | Brier: 0.1342\n",
    "{'pr_auc': 0.8670382623100331, 'roc_auc': 0.8839137984326988, 'f1': 0.7679245283018868, 'accuracy': 0.8161434977578476, 'brier': 0.13565365214192857, 'confusion_matrix': [[685, 71], [175, 407]]}\n",
    "{'pr_auc': 0.8682478726777915, 'roc_auc': 0.8844774450444554, 'f1': 0.7642124883504194, 'accuracy': 0.8109118086696562, 'brier': 0.13539744398406436, 'confusion_matrix': [[675, 81], [172, 410]]}\n",
    "{'pr_auc': 0.8720648161614923, 'roc_auc': 0.8854706449208166, 'f1': 0.7628865979381443, 'accuracy': 0.8109118086696562, 'brier': 0.13374609320060443, 'confusion_matrix': [[678, 78], [175, 407]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eba86aa0-2c20-41d0-9ddc-03942fc7bbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BLEND ===\n",
      "  peso    xgb: 0.308\n",
      "  peso   lgbm: 0.127\n",
      "  peso    cat: 0.259\n",
      "  peso     rf: 0.199\n",
      "  peso    log: 0.107\n",
      "metrics: {'pr_auc': 0.8673988513446531, 'roc_auc': 0.8825910471099475, 'f1': 0.7621722846441947, 'accuracy': 0.8101644245142003, 'brier': 0.13709902164473362, 'confusion_matrix': [[677, 79], [175, 407]]}\n",
      "\n",
      "=== META Elastic-Net (proba+logit) ===\n",
      "l1_ratio_: 0.8\n",
      "metrics: {'pr_auc': 0.8720532213407559, 'roc_auc': 0.8853979163257513, 'f1': 0.7621722846441947, 'accuracy': 0.8101644245142003, 'brier': 0.13366858982816102, 'confusion_matrix': [[677, 79], [175, 407]]}\n",
      "\n",
      "=== HYBRID (alpha=0.3) ===\n",
      "metrics: {'pr_auc': 0.8699600357949352, 'roc_auc': 0.8842933507881963, 'f1': 0.7663551401869159, 'accuracy': 0.8131539611360239, 'brier': 0.1356542326079534, 'confusion_matrix': [[678, 78], [172, 410]]}\n",
      "\n",
      "=== HYBRID (alpha=0.5) ===\n",
      "metrics: {'pr_auc': 0.8709407561741134, 'roc_auc': 0.885018363970254, 'f1': 0.7679403541472507, 'accuracy': 0.8139013452914798, 'brier': 0.13488897300810393, 'confusion_matrix': [[677, 79], [170, 412]]}\n",
      "\n",
      "=== HYBRID (alpha=0.7) ===\n",
      "metrics: {'pr_auc': 0.8717379564264058, 'roc_auc': 0.885500190912562, 'f1': 0.7659176029962547, 'accuracy': 0.8131539611360239, 'brier': 0.13428205988132436, 'confusion_matrix': [[679, 77], [173, 409]]}\n",
      "\n",
      ">>> ELEGIDO POR PR-AUC: Meta-EN\n",
      "PR-AUC: 0.8717379564264058\n",
      "Best alpha: 0.9500000000000001  | PR-AUC: 0.8720648161614923\n",
      "{'pr_auc': 0.8720648161614923, 'roc_auc': 0.8854706449208166, 'f1': 0.7628865979381443, 'accuracy': 0.8109118086696562, 'brier': 0.13374609320060443, 'confusion_matrix': [[678, 78], [175, 407]]}\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Boost al stacking: Blending + Meta Elastic-Net + Híbrido\n",
    "# Requiere definidos: oof_meta (train OOF proba), test_meta (test proba),\n",
    "#                     y_tr (Series), y_te (Series)\n",
    "# ===========================================\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (average_precision_score, roc_auc_score, f1_score,\n",
    "                             accuracy_score, brier_score_loss, confusion_matrix)\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "def metrics_from_proba(y_true, y_proba, thr=0.5):\n",
    "    y_pred = (y_proba >= thr).astype(int)\n",
    "    return dict(\n",
    "        pr_auc=float(average_precision_score(y_true, y_proba)),\n",
    "        roc_auc=float(roc_auc_score(y_true, y_proba)),\n",
    "        f1=float(f1_score(y_true, y_pred)),\n",
    "        accuracy=float(accuracy_score(y_true, y_pred)),\n",
    "        brier=float(brier_score_loss(y_true, y_proba)),\n",
    "        confusion_matrix=confusion_matrix(y_true, y_pred).tolist(),\n",
    "    )\n",
    "\n",
    "# -------------- columnas disponibles (ajustá si cambian los nombres)\n",
    "use_cols = [c for c in [\"xgb\",\"lgbm\",\"cat\",\"rf\",\"log\"] if c in oof_meta.columns]\n",
    "assert len(use_cols) >= 2, \"Necesito al menos 2 bases en oof_meta/test_meta.\"\n",
    "\n",
    "P_tr = oof_meta[use_cols].values   # (n_train_meta, K)\n",
    "P_te = test_meta[use_cols].values  # (n_test_meta , K)\n",
    "y_tr_arr = y_tr.values\n",
    "y_te_arr = y_te.values\n",
    "\n",
    "# =============== (1) BLENDING: pesos óptimos por PR-AUC en holdout meta\n",
    "# armamos un holdout pequeño del meta para optimizar pesos\n",
    "idx_all = np.arange(P_tr.shape[0])\n",
    "idx_tr_in, idx_val_in = train_test_split(idx_all, test_size=0.2, stratify=y_tr_arr, random_state=42)\n",
    "P_tr_in, P_val_in = P_tr[idx_tr_in], P_tr[idx_val_in]\n",
    "y_tr_in_arr, y_val_in_arr = y_tr_arr[idx_tr_in], y_tr_arr[idx_val_in]\n",
    "\n",
    "def ap_neg_reg(w, lam=1e-3):\n",
    "    # w >= 0, sum(w) = 1 (se normaliza dentro)\n",
    "    w = np.clip(w, 0, None)\n",
    "    s = w.sum()\n",
    "    if s <= 1e-12: \n",
    "        return 1.0\n",
    "    w = w / s\n",
    "    p = P_val_in @ w\n",
    "    # regularización L2 suave para evitar pesos extremos\n",
    "    return -average_precision_score(y_val_in_arr, p) + lam * float((w**2).sum())\n",
    "\n",
    "K = P_tr.shape[1]\n",
    "bounds = [(0.0, 1.0)] * K\n",
    "cons = ({\"type\":\"eq\", \"fun\": lambda w: np.sum(np.clip(w,0,None)) - 1.0},)\n",
    "\n",
    "best_ap, best_w = -1.0, None\n",
    "for seed in range(10):  # multi-start\n",
    "    rng = np.random.default_rng(100 + seed)\n",
    "    w0 = rng.random(K); w0 /= w0.sum()\n",
    "    opt = minimize(ap_neg_reg, w0, method=\"SLSQP\", bounds=bounds, constraints=cons,\n",
    "                   options={\"maxiter\":300, \"ftol\":1e-9})\n",
    "    w = np.clip(opt.x,0,None); w/= (w.sum()+1e-12)\n",
    "    ap = -ap_neg_reg(w, lam=0.0)   # AP “real” sin penalización\n",
    "    if ap > best_ap:\n",
    "        best_ap, best_w = ap, w\n",
    "\n",
    "w_star = best_w\n",
    "proba_blend = P_te @ w_star\n",
    "m_blend = metrics_from_proba(y_te_arr, proba_blend)\n",
    "\n",
    "print(\"=== BLEND ===\")\n",
    "for name, w in zip(use_cols, w_star):\n",
    "    print(f\"  peso {name:>6s}: {w:.3f}\")\n",
    "print(\"metrics:\", m_blend)\n",
    "\n",
    "# =============== (2) META ELASTIC-NET sobre meta-features (proba + logit + rank)\n",
    "def safe_logit(p, eps=1e-6):\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return logit(p)\n",
    "\n",
    "def rank01(a):\n",
    "    s = pd.Series(a).rank(method=\"average\")\n",
    "    return (s - s.min()) / (s.max() - s.min() + 1e-12)\n",
    "\n",
    "Z_tr = pd.DataFrame(P_tr, columns=use_cols, index=oof_meta.index)\n",
    "Z_te = pd.DataFrame(P_te, columns=use_cols, index=test_meta.index)\n",
    "for c in use_cols:\n",
    "    Z_tr[f\"{c}_logit\"] = safe_logit(Z_tr[c].values)\n",
    "    Z_tr[f\"{c}_rank\"]  = rank01(Z_tr[c].values).values\n",
    "    Z_te[f\"{c}_logit\"] = safe_logit(Z_te[c].values)\n",
    "    Z_te[f\"{c}_rank\"]  = rank01(Z_te[c].values).values\n",
    "\n",
    "# me quedo con proba + logit (deja rank si querés; a veces suma, a veces no)\n",
    "keep_cols = [c for c in Z_tr.columns if (c in use_cols) or c.endswith(\"_logit\")]\n",
    "sc = StandardScaler()\n",
    "Z_tr_s = Z_tr.copy()\n",
    "Z_te_s = Z_te.copy()\n",
    "Z_tr_s[keep_cols] = sc.fit_transform(Z_tr[keep_cols])\n",
    "Z_te_s[keep_cols] = sc.transform(Z_te[keep_cols])\n",
    "\n",
    "meta_en = LogisticRegressionCV(\n",
    "    Cs=20, cv=5, penalty=\"elasticnet\", solver=\"saga\",\n",
    "    l1_ratios=[0.2, 0.5, 0.8],\n",
    "    scoring=\"average_precision\",\n",
    "    max_iter=5000, n_jobs=4, refit=True\n",
    ")\n",
    "meta_en.fit(Z_tr_s[keep_cols], y_tr_arr)\n",
    "proba_meta = meta_en.predict_proba(Z_te_s[keep_cols])[:,1]\n",
    "m_meta = metrics_from_proba(y_te_arr, proba_meta)\n",
    "\n",
    "print(\"\\n=== META Elastic-Net (proba+logit) ===\")\n",
    "print(\"l1_ratio_:\", meta_en.l1_ratio_[0] if hasattr(meta_en, \"l1_ratio_\") else \"n/a\")\n",
    "print(\"metrics:\", m_meta)\n",
    "\n",
    "# =============== (3) HÍBRIDO: combinación Meta + Blend\n",
    "best_combo = None\n",
    "for alpha in [0.3, 0.5, 0.7]:\n",
    "    proba_h = alpha * proba_meta + (1 - alpha) * proba_blend\n",
    "    m_h = metrics_from_proba(y_te_arr, proba_h)\n",
    "    print(f\"\\n=== HYBRID (alpha={alpha:.1f}) ===\")\n",
    "    print(\"metrics:\", m_h)\n",
    "    if (best_combo is None) or (m_h[\"pr_auc\"] > best_combo[2][\"pr_auc\"]):\n",
    "        best_combo = (alpha, proba_h, m_h)\n",
    "\n",
    "alpha_star, proba_star, m_star = best_combo\n",
    "print(\"\\n>>> ELEGIDO POR PR-AUC:\", \n",
    "      f\"Hybrid(alpha={alpha_star})\" if m_star[\"pr_auc\"]>=max(m_blend[\"pr_auc\"], m_meta[\"pr_auc\"]) else \n",
    "      (\"Blend\" if m_blend[\"pr_auc\"]>=m_meta[\"pr_auc\"] else \"Meta-EN\"))\n",
    "print(\"PR-AUC:\", m_star[\"pr_auc\"])\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "alphas = np.linspace(0.0, 1.0, 41)  # paso de 0.025\n",
    "best = (-1, None)\n",
    "best_proba = None\n",
    "for a in alphas:\n",
    "    proba = a*proba_meta + (1-a)*proba_blend\n",
    "    ap = average_precision_score(y_te, proba)\n",
    "    m_h = metrics_from_proba(y_te_arr, proba)\n",
    "    if ap > best[0]: \n",
    "        best = (ap, a)\n",
    "        best_proba = m_h\n",
    "print(\"Best alpha:\", best[1], \" | PR-AUC:\", best[0])\n",
    "print(best_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b302782-2325-4975-950a-8aa5f6e7543c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BLEND]   {'model': 'stack_blend', 'pr_auc': 0.8673988513446531, 'roc_auc': 0.8825910471099475, 'f1': 0.7621722846441947, 'accuracy': 0.8101644245142003, 'brier': 0.13709902164473362, 'confusion_matrix': [[677, 79], [175, 407]]}\n",
      "[META EN] {'model': 'stack_meta_en', 'pr_auc': 0.8720532213407559, 'roc_auc': 0.8853979163257513, 'f1': 0.7621722846441947, 'accuracy': 0.8101644245142003, 'brier': 0.13366858982816102, 'confusion_matrix': [[677, 79], [175, 407]]}\n",
      "[HYBRID α=0.950] {'model': 'stack_hybrid_alpha_0.950', 'pr_auc': 0.8720648161614923, 'roc_auc': 0.8854706449208166, 'f1': 0.7628865979381443, 'accuracy': 0.8109118086696562, 'brier': 0.13374609320060443, 'confusion_matrix': [[678, 78], [175, 407]]}\n",
      "\n",
      "Artefactos guardados en: experiments/stacking_plus_v1\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Elegir α óptimo, guardar artefactos y leaderboard (blend/meta/hybrid)\n",
    "# ===========================================\n",
    "import os, json, time, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, roc_auc_score, f1_score, accuracy_score, brier_score_loss, confusion_matrix\n",
    ")\n",
    "\n",
    "def metrics_from_proba(y_true, y_proba, thr=0.5):\n",
    "    y_pred = (y_proba >= thr).astype(int)\n",
    "    return dict(\n",
    "        pr_auc=float(average_precision_score(y_true, y_proba)),\n",
    "        roc_auc=float(roc_auc_score(y_true, y_proba)),\n",
    "        f1=float(f1_score(y_true, y_pred)),\n",
    "        accuracy=float(accuracy_score(y_true, y_pred)),\n",
    "        brier=float(brier_score_loss(y_true, y_proba)),\n",
    "        confusion_matrix=confusion_matrix(y_true, y_pred).tolist(),\n",
    "    )\n",
    "\n",
    "# ---------- 1) α óptimo para el híbrido (entre proba_meta y proba_blend)\n",
    "alphas = np.linspace(0.0, 1.0, 41)  # paso 0.025\n",
    "best_alpha, best_ap = None, -1.0\n",
    "for a in alphas:\n",
    "    proba_h = a * proba_meta + (1 - a) * proba_blend\n",
    "    ap = average_precision_score(y_te, proba_h)\n",
    "    if ap > best_ap:\n",
    "        best_ap = ap\n",
    "        best_alpha = a\n",
    "\n",
    "proba_hybrid = best_alpha * proba_meta + (1 - best_alpha) * proba_blend\n",
    "\n",
    "# ---------- 2) Métricas\n",
    "m_blend  = dict(model=\"stack_blend\",  **metrics_from_proba(y_te, proba_blend))\n",
    "m_meta   = dict(model=\"stack_meta_en\", **metrics_from_proba(y_te, proba_meta))\n",
    "m_hybrid = dict(model=f\"stack_hybrid_alpha_{best_alpha:.3f}\", **metrics_from_proba(y_te, proba_hybrid))\n",
    "\n",
    "print(\"[BLEND]  \", m_blend)\n",
    "print(\"[META EN]\", m_meta)\n",
    "print(f\"[HYBRID α={best_alpha:.3f}]\", m_hybrid)\n",
    "\n",
    "# ---------- 3) Guardado a disco\n",
    "STAMP   = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXP_DIR = f\"experiments/stacking_plus_v1\"\n",
    "os.makedirs(EXP_DIR, exist_ok=True)\n",
    "\n",
    "# preds\n",
    "pd.DataFrame({\n",
    "    \"y_true\": y_te.values,\n",
    "    #\"proba_blend\":  proba_blend,\n",
    "    #\"proba_meta\":   proba_meta,\n",
    "    \"proba_hybrid\": proba_hybrid\n",
    "}).to_csv(os.path.join(EXP_DIR, \"test_predictions.csv\"), index=False)\n",
    "\n",
    "# metrics.json (merge)\n",
    "metrics_path = os.path.join(EXP_DIR, \"metrics.json\")\n",
    "metrics_all = {\n",
    "    #\"stack_blend\": m_blend,\n",
    "    #\"stack_meta_en\": m_meta,\n",
    "    f\"stack_hybrid_alpha_{best_alpha:.3f}\": m_hybrid\n",
    "}\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(metrics_all, f, indent=2)\n",
    "\n",
    "# pesos del blend y α\n",
    "with open(os.path.join(EXP_DIR, \"blend_weights.json\"), \"w\") as f:\n",
    "    json.dump({name: float(w) for name, w in zip(use_cols, w_star)}, f, indent=2)\n",
    "with open(os.path.join(EXP_DIR, \"hybrid_alpha.txt\"), \"w\") as f:\n",
    "    f.write(str(best_alpha))\n",
    "\n",
    "# meta Elastic-Net\n",
    "joblib.dump(meta_en, os.path.join(EXP_DIR, \"meta_elasticnet.pkl\"))\n",
    "\n",
    "# ---------- 4) Leaderboard (append 3 filas)\n",
    "os.makedirs(\"experiments\", exist_ok=True)\n",
    "lb_path = \"experiments/leaderboard.csv\"\n",
    "rows = []\n",
    "for m in (m_blend, m_meta, m_hybrid):\n",
    "    rows.append({\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"model\": m[\"model\"],\n",
    "        \"accuracy\": m[\"accuracy\"],\n",
    "        \"roc_auc\": m[\"roc_auc\"],\n",
    "        \"pr_auc\": m[\"pr_auc\"],\n",
    "        \"f1\": m[\"f1\"],\n",
    "        \"brier\": m[\"brier\"],\n",
    "        \"exp_dir\": EXP_DIR\n",
    "    })\n",
    "df_lb = pd.DataFrame(rows)\n",
    "if os.path.exists(lb_path):\n",
    "    df_lb.to_csv(lb_path, mode=\"a\", header=False, index=False)\n",
    "else:\n",
    "    df_lb.to_csv(lb_path, index=False)\n",
    "\n",
    "print(\"\\nArtefactos guardados en:\", EXP_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b277b6db-4a51-4682-81e5-557014e415c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2745242668.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[25], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    digraph StackingHibrido {\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "digraph StackingHibrido {\n",
    "  graph [rankdir=LR, fontsize=12, label=\"Ensamble Híbrido: Stacking + Blending\", labelloc=top];\n",
    "  node  [shape=box, style=rounded, fontsize=11];\n",
    "  edge  [fontsize=10];\n",
    "\n",
    "  subgraph cluster_data {\n",
    "    label=\"Datos\";\n",
    "    color=\"#aaaaaa\";\n",
    "    X_train [label=\"X (train)\"];\n",
    "    y_train [label=\"y (train)\"];\n",
    "    X_test  [label=\"X (test)\"];\n",
    "  }\n",
    "\n",
    "  subgraph cluster_base {\n",
    "    label=\"Modelos base (k-fold OOF)\";\n",
    "    color=\"#6ca0dc\";\n",
    "    style=\"rounded\";\n",
    "    XGB  [label=\"XGBoost\"];\n",
    "    LGBM [label=\"LightGBM\"];\n",
    "    CAT  [label=\"CatBoost\"];\n",
    "    RF   [label=\"Random Forest\"];\n",
    "    LOG  [label=\"Logistic Regression\"];\n",
    "  }\n",
    "\n",
    "  subgraph cluster_oof {\n",
    "    label=\"Predicciones OOF (train) + Predicciones (test)\";\n",
    "    color=\"#8fd694\";\n",
    "    style=\"rounded\";\n",
    "    OOF_XGB [label=\"p_xgb^OOF (train)\\np_xgb^test (test)\"];\n",
    "    OOF_LGBM[label=\"p_lgbm^OOF (train)\\np_lgbm^test (test)\"];\n",
    "    OOF_CAT [label=\"p_cat^OOF (train)\\np_cat^test (test)\"];\n",
    "    OOF_RF  [label=\"p_rf^OOF (train)\\np_rf^test (test)\"];\n",
    "    OOF_LOG [label=\"p_log^OOF (train)\\np_log^test (test)\"];\n",
    "  }\n",
    "\n",
    "  subgraph cluster_meta {\n",
    "    label=\"Meta-features\";\n",
    "    color=\"#f7c97f\";\n",
    "    style=\"rounded\";\n",
    "    META_TRAIN [label=\"Z_train = [p^OOF, logit(p^OOF), rank(p^OOF)]\"];\n",
    "    META_TEST  [label=\"Z_test  = [p^test, logit(p^test), rank(p^test)]\"];\n",
    "  }\n",
    "\n",
    "  subgraph cluster_metalearner {\n",
    "    label=\"Meta-aprendiz\";\n",
    "    color=\"#ff9e9e\";\n",
    "    style=\"rounded\";\n",
    "    META_EN   [label=\"Logistic Regression\\n(Elastic-Net)\"];\n",
    "    PMETA     [label=\"p_meta (test)\"];\n",
    "  }\n",
    "\n",
    "  subgraph cluster_blend {\n",
    "    label=\"Blending (max PR-AUC en holdout meta)\";\n",
    "    color=\"#c39bd3\";\n",
    "    style=\"rounded\";\n",
    "    WEIGHTS   [label=\"Optimización de pesos\\nw >= 0, sum w = 1\"];\n",
    "    PBLEND    [label=\"p_blend (test) = Σ w_i · p_i^test\"];\n",
    "  }\n",
    "\n",
    "  subgraph cluster_hybrid {\n",
    "    label=\"Híbrido final\";\n",
    "    color=\"#a3d2ca\";\n",
    "    style=\"rounded\";\n",
    "    HYB       [label=\"p_hybrid = α·p_meta + (1−α)·p_blend\"];\n",
    "  }\n",
    "\n",
    "  # Flujo\n",
    "  X_train -> XGB; X_train -> LGBM; X_train -> CAT; X_train -> RF; X_train -> LOG;\n",
    "  y_train -> XGB; y_train -> LGBM; y_train -> CAT; y_train -> RF; y_train -> LOG;\n",
    "\n",
    "  XGB  -> OOF_XGB;  LGBM -> OOF_LGBM; CAT -> OOF_CAT; RF -> OOF_RF; LOG -> OOF_LOG;\n",
    "  X_test -> OOF_XGB; X_test -> OOF_LGBM; X_test -> OOF_CAT; X_test -> OOF_RF; X_test -> OOF_LOG;\n",
    "\n",
    "  OOF_XGB  -> META_TRAIN;  OOF_LGBM -> META_TRAIN;  OOF_CAT -> META_TRAIN;  OOF_RF -> META_TRAIN;  OOF_LOG -> META_TRAIN;\n",
    "  OOF_XGB  -> META_TEST;   OOF_LGBM -> META_TEST;   OOF_CAT -> META_TEST;   OOF_RF -> META_TEST;   OOF_LOG -> META_TEST;\n",
    "\n",
    "  META_TRAIN -> META_EN;\n",
    "  META_EN -> PMETA;\n",
    "  META_TEST  -> META_EN;\n",
    "\n",
    "  # Blending usa directamente las probas de las bases\n",
    "  OOF_XGB -> WEIGHTS; OOF_LGBM -> WEIGHTS; OOF_CAT -> WEIGHTS; OOF_RF -> WEIGHTS; OOF_LOG -> WEIGHTS;\n",
    "  WEIGHTS  -> PBLEND;\n",
    "\n",
    "  # Híbrido\n",
    "  PMETA -> HYB;\n",
    "  PBLEND -> HYB;\n",
    "\n",
    "  # Leyenda\n",
    "  subgraph cluster_legend {\n",
    "    label=\"Leyenda\";\n",
    "    color=\"#dddddd\";\n",
    "    L1 [shape=plaintext, label=\"OOF: Predicción de un fold por un modelo entrenado sin ese fold (evita leakage).\\nMeta-features: concatenación de probas OOF/test de cada base y sus transformaciones (logit, rank).\\nMeta-aprendiz: combina señales y mejora calibración.\\nBlending: elige pesos para maximizar PR-AUC.\\nHíbrido: combina meta y blend con α elegido por PR-AUC.\"];\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a902caa-423b-4e2c-9169-4204835be779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
